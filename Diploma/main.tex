% !TeX spellcheck = sl_SI
\documentclass[11pt,a4paper,openany]{book}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, fullpage}
\usepackage{lmodern}
\usepackage{epsfig}
\usepackage{setspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{parskip}
\usepackage{wrapfig}
\usepackage{footnote}
% \usepackage[dvips]{graphicx}
\usepackage[titletoc]{appendix}
\usepackage[dvips]{xy}
\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{soul}
\onehalfspacing
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\usepackage[]{algorithm2e}

\begin{document}
\input{macros}

% -----------------------------------------------------------------------------
% UVODNESTRANI
% -----------------------------------------------------------------------------
\input{intropages}







% -----------------------------------------------------------------------------
% POVZETEK
% -----------------------------------------------------------------------------
\chapter*{Povzetek}
\addcontentsline{toc}{chapter}{Povzetek}

Plan dela:
\begin{itemize}
	\item \st{Uvod} + Opis HiggsML + Osnovno razumevanje fizikalnega dela + opis
	\item \st{Exploratory analysis + opis podatkov}
	\item Usposobitev baseline (predlaganih) metod
		\begin{itemize}
			\item \st{Simple Window}
			\item \st{Logisticna regresija}
			\item Naive Bayes + opis metode
			\item XGBoost + opis nagrajene metode
		\end{itemize}
	\item Osnove strojnega učenja + delo na ostalih metodah + opis metod
		\begin{itemize}
			\item \st{Logistic regression}
			\item \st{Perceptron}
			\item NN + opis zmagovalne metode
			\item SVM
		\end{itemize}
	\item Izboljšava metod
		\begin{itemize}			
			\item Na podlagi SVM
		\end{itemize}
	\item Zaključek + povzetek
\end{itemize} 

\vspace{1.3cm}
\noindent
{\large \bf Ključne besede:}

\vspace{0.5cm}
\noindent test, test, test


Abstract. Key words.







% -----------------------------------------------------------------------------
% UVOD
% -----------------------------------------------------------------------------
\chapter*{Uvod}
\addcontentsline{toc}{chapter}{Uvod}

Eksperimenta ATLAS in CMS sta leta 2012 objavila odkritje Higgsovega bozona\cite{Aad20121,Chatrchyan201230}. Odkritju je leta 2013 sledila Nobelova nagrada za fiziko, ki sta jo prejela François Englert in Peter Higgs. Obstoj delca, katerega vloga naj bi bila, da daje maso ostalim elementarnim delcem, je bil predviden pred skoraj 50 leti. Eksperimenti so potekali (in še vedno potekajo) na Velikem hadronskem trkalniku (Large Hadron Collider - LHC) v CERN-u (Evropski organizaciji za jedrske raziskave) v Ženevi\cite{ChallengeDoc}.

Higgsov bozon lahko razpade skozi različne procese, ki jim v fiziki osnovnih delcev pravimo kanali. Pri tem nastanejo novi delci. Higgsov bozon so najprej opazili v treh različnih razpadnih kanalih, v katerih vedno nastanejo pari bozonov. V naslednjem koraku je bilo potrebno najti dokaze o razpadu Higgsovega bozona v fermionske pare, predvsem v $\tau$ leptone in $b$ kvarke. Prvi dokazi o $H \rightarrow \tau^+\tau^-$ so bili predstavljeni v \cite{atlas2013}. 

\comment{Ali so rezultati vpliva $H \rightarrow \tau^+\tau^-$ že kje? Se to kaj navede? Koliko so izboljšali $\sigma$?}

Pri analizi eksperimentalnih podatkov je potrebno določiti relevantno območje faznega prostora izmerjenih značilk, v katerem je velika verjetnost, da smo naleteli na dogodke, ki nas zanimajo (v našem primeru na razpad $H \rightarrow \tau^+\tau^-$). V preteklosti so ta področja določali eksperti \textit{ročno}\cite{Adam-Bourdarios14}. Napredne klasifikacijske metode, ki temeljijo na strojnem učenju, pa se danes rutinirano uporabljajo za reševanje tega in podobnih problemov\cite{atlas2013}.

Področje dela tesno povezuje fiziko z računalništvom, natančneje - s strojnim učenjem. 

Namen diplomskega dela je predvsem seznaniti se z uporabo metod strojnega učenja pri ločevanju signala in ozadja pri razpadu $H \rightarrow \tau^+\tau^-$, preveriti in ovrednotiti različne klasifikacijske metode na podatkih simulatorja ATLAS in preveriti proces optimizacije teh metod, ki izhaja iz podatkov (in ne nujno domenskega znanja). Sekundarni cilj je razvoj lastne klasifikacijske metode, ki bo temeljila na metodi podpornih vektorjev (SVM).

Še enkrat velja izpostaviti, da se diplomsko delo nanaša predvsem na uporabo metod strojnega učenja pri analizi zahtevnih in analitično neobvladljivih problemov, in ne na fiziko osnovnih delcev in razlago standardnega modela (SM).

\comment{Če zmagovalna metoda na lepih podatkih doseže AMS 3,7, kako lahko na podlagi teh}
\comment{rezultatov pridemo do $\sigma = 5$??}

\section*{Izziv HiggsML}
\comment{Opis challenge-a.}
\comment{Globoko fizikalno razumevanje načeloma v tem izzivu ni pripomoglo k signifikantnemu izboljšanju}
\comment{klasifikacijskih modelov. Osnovne izpeljane vrednosti so zadoščale!}
\comment{Vir: https://www.kaggle.com/c/higgs-boson/forums/t/10350/how-physicists-fared}


\section*{Terminologija}
\addcontentsline{toc}{section}{Terminologija}

Ker je področje strojnega učenja in umetne inteligence v slovenskem prostoru majhno in ker se slovenska terminologija na tem področju še redkeje uporablja in je zato tudi strokovno podkovanemu bralcu manj znana, na tem mestu prilagamo kratek slovarček strokovnih terminov s področja strojnega učenja. Vsi prevodi temeljijo na \textit{Računalniškem slovarčku}, ki ga vzdržuje Odsek za inteligentne sistem na Inštitutu "Jožef Stefan"\footnote{\url{http://dis-slovarcek.ijs.si}}.

\input{slovar}

% -----------------------------------------------------------------------------
% POGLAVJE: Fizikalne osnove
% -----------------------------------------------------------------------------
\chapter{Fizikalne osnove}

\comment{Osnovni članek: \cite{AadScience2012}.}


\section{Razpad $H \rightarrow \tau^+\tau^-$}

\cite{Baldi2014} \cite{atlas2013}


\section{ATLAS detektor}
\cite{AadScience2012}


\section{Izpeljava relevantnih značilk}
Podpoglavje opisuje nekaj osnovnih enačb iz posebne teorije relativnosti in sledi predvsem izpeljavi novih značilk iz surovih podatkov. Izpeljane značilke si je moč ogledati v razdelku \ref{sec:opis_podatkov}.

\subsection{Gibalna količina, masa in energija}
Fundamentalna enačba posebne teorije relativnostni
\begin{equation}
	E^2 = p^2c^2 + m^2c^4,
	\label{stirimoment}
\end{equation}
\comment{Strnad 3/39 (9)}
kjer je $E$ energija delca, $p$ njegova gibalna količina, $m$ lastna masa (masa v mirovanju) in $c$ hitrost svetlobe v praznem prostoru. V primeru, ko delec miruje (in ima s tem gibalno količino enako $0$), se enačba poenostavi v znano $E = mc^2$.

\comment{Relevantno za izpeljavo značilk! Navedba/izpeljava enačb za izpeljane vrednosti ... \cite{Adam-Bourdarios14}}
\comment{Mogoče smiselno citirati tudi Strnada 3?}

% -----------------------------------------------------------------------------
% POGLAVJE: Opredelitev problema
% -----------------------------------------------------------------------------
\chapter{Opredelitev problema}

Problem, s katerim sem se spopadel v diplomskem delu, sledi formulaciji na odprtem tekmovanju \textit{The Higgs Boson Machine Learning Challenge (HiggsML)}\footnote{ \url{http://www.kaggle.com/c/higgs-boson}}, ki temelji na rezultatih kolaboracije ATLAS\cite{Adam-Bourdarios14}. 


\section{Formalna opredelitev problema}

\comment{Notacijo spremeni, da bo konsistentna z Andrew Ng - spodaj.}

Naj ${\cal D} = \left\{({\mathbf x}_1, y_1, w_1), \dots, ({\mathbf x}_n, y_n, w_n) \right\}$ predstavlja učno podatkovno množico, kjer je $\mathbf{x}_i \in \mathbb{R}^d$ $d$-dimenzionalni vektor značilk, $y_i \in \{\text{b, s}\}$ je oznaka, $w_i \in \mathbb{R}^+$ pa je nenegativna utež. Naj bosta ${\cal S} = \{i : y_i = \text{s}\}$ in ${\cal B} = \{i : y_i = \text{b}$ množici indeksov dogodkov, ki predstavljajo signal in ozadje, $n_\text{s} = |{\cal S}|$ in $n_\text{b} = |{\cal B}|$ pa naj označujeta števili simuliranih dogodkov, ki predstavljata signal in ozadje.

Podatki, na katerih se učimo, so simulirani (glej poglavje \ref{analiza-podatkov}) in se razlikujejo od izmerjenih. Razmerje $n_\text{s} / n_\text{b}$ v podatkih tako ne odraža dejanskega razmerja dogodkov $P(y = s) / P(y = b)$. Glede ne nizko verjetnost, da pri nekem naravnem dogodku gre za signal \cite{Adam-Bourdarios14}, je tako učna podatkovna množica precej bolj uravnotežena in omogoča metodam, da se lahko naučijo razlikovati med dogodki, ki predstavljajo ozadje in tistimi, ki predstavljajo signal.

Podatek, ki ga da simulacija, je tudi utež $w_i$, ki je mera za pomembnost nekega simuliranega dogodka. Ker je optimizacijska funkcija (\ref{en:ams}) odvisna od \textit{nenormalizirane vsote} uteži in ker želimo, da je naš sistem invarianten na števili simuliranih dogodkov $n_s$ in $n_b$, moramo vsoto vsake podatkovne množice (učne, validacijske ...) za vsak razred (signal ali ozadje) fiksirano:
\begin{equation}
\sum_{i \in \cal{S}}{w_i} = N_s
\qquad\text{in}\qquad
\sum_{i \in \cal{B}}{w_i} = N_b
\end{equation}
Normalizacijski konstanti $N_s$ in $N_b$ imata fizikalni pomen in predstavljata \textit{pričakovano število} dogodkov ki predstavljajo signal in ozadje, med intervalom zajema podatkov (v našem primeru leta 2012). Individualne uteži so proporcionalne pogojnim gostotam, deljenimi z instrumentalnimi gostotami, uporabljenimi na simulatorju.

\begin{equation}
	w_i = \left\{\begin{array}{r}
		p_s(\textbf{x}_i)/q_s(x_i),\qquad \text{if}\;y_i = s \\
		p_b(\textbf{x}_i)/q_b(x_i),\qquad \text{if}\;y_i = b 
	\end{array}
	\right.,
\end{equation}
kjer sta
\begin{equation*}
p_s(\textbf{x}_i) = p(\textbf{x}_i|y = s) \qquad in \qquad p_b(\textbf{x}_i) = p(\textbf{x}_i|y = b)
\end{equation*}
pogojni gostoti signala in ozadja ter $q_s(\textbf{x}_i)$ in $q_b(\textbf{x}_i)$ instrumentalni gostoti.

Naj bo $g : \mathbb{R}^d \rightarrow \{b, s\}$ poljubna klasifikacijska funkcija. Naj bo izbrano področje ${\cal G} = \{\textbf{x} : g(\textbf{x}) = s\}$ množca točk, klasificirana kod signal, in naj $\hat{\cal G}$ označuje množico indeksov točk, ki jih $g$ izbere (klasificira kot signal).
\begin{equation*}
\hat{\cal G} = \{ i : \textbf{x}_i \in {\cal G}\} = \{ i : g(\textbf{x}_i) = s \}.
\end{equation*}


\section{AMS metrika}
\label{sc:ams}
\begin{equation}
\text{AMS} = \sqrt{2 \left( ( s + b + b_{reg} ) \ln \left( 1 +  \frac{s}{b + b_{reg}} \right) - s \right) }
\label{en:ams}
\end{equation}

% -----------------------------------------------------------------------------
% POGLAVJE: Metode strojnega učenja
% -----------------------------------------------------------------------------
\chapter{Strojno učenje}

\cite{Mitchell1997}, \cite{Witten2005}.

\section{Raziskovalna analiza podatkov}
Raziskovalna analiza podatkov predstavlja enega od pristopov k začetni analizi množice podatkov predvsem s pomočjo vizualnih metod. Vizualne metode temeljijo na sposobnosti človeških možganov, da prepoznajo strukturo v podatkih. Namen analize je pregledati osnovne lastnosti podatkov, ki pomagajo pri določanju scenarijev za implementacijo metod strojnega učenja. Poleg omenjenega so vizualne metode na področju rudarjenja podatkov pomembne tudi zato, ker je mogoče z njihovo pomočjo odkriti nove, nepričakovane zakonitosti v podatkih. Metode raziskovalne analize imajo pri visoko-dimenzionalnih naborih podatkov očitne omejitve. Opisi v diplomskem delu so povzeti po \cite{hand2001}.

Metoda odraža pristop k reševanju problema, ki temelji na podatkih in ne na predhodnem znanju. S pomočjo takega pristopa lahko odkrijemo nove značilnosti v podatkih in s tem novo znanje. Metodologija je bila razvita že v začetku šestdesetih let prejšnjega stoletja, je pa svoj preporod doživela z razvojem računalništva, ki je močno olajšalo izdelavo vizualizacij. 

\textbf{Osnovni pregled značilk} se nanaša na pregled po eni posamezni značilki. Zajema \textbf{pregled manjkajočih vrednosti} v podatkih, \textbf{pregled osnovnih agregatov} in natančnejši \textbf{pregled porazdelitev} določenih značilk s pomočjo histogramov. Pregled osnovnih agregatov zajema: povprečja, standardno deviacijo, minimum, maksimum, število vrednosti, kvantile ... Ponazoritev posamezhnih značilk s pomočjo histogramov pa nam da lepši grafični pregled in informacijo o porazdelitvi posamezne značilke.

\textbf{Razpršeni grafi} se nanašajo na 2 značilki. Podatke ponazorimo s točkami, katerih eno koordinato predstavlja vrednost ene, drugo pa druge značilke. Grafe ponavadi prikažemo v matriki 2, 3 ali 4 različnih značilk, tako da jih lahko primerjamo med sabo. Takšen prikaz nam lahko razkrije skrito strukturo v podatkih, ki je po pregledu ene same značilke ne zasledimo. V primeru klasifikacije lahko npr. opazimo, če določena kombinacija značilk bolje loči oba razreda kot ena sama, prav tako pa lahko s temi grafi opazimo medsebojno korelacijo posameznih značilk, ki se odraža na urejenosti/razpršenosti točk.

\textbf{Korelacijska matrika značilk} je zelo učinkovita metoda za hiter in celosten pregled nad podatki. V matriki dimenzije $d \times d$, kjer je $d$ število značilk, s pomočjo barvne lestvice ponazorimo korelacijo med parom značilk. S pomočjo kompletnega pivotiranja, ki ga naredimo s pomočjo hierarhičnega grozdenja značilk, lahko značilke, ki so si med seboj podobne glede na korelacijo z drugimi, smiselno uredimo. Metoda je še zlasti uporabna pri izločanju linearno odvisnih značilk, ki modelom ne prinašajo dodatne vrednosti. 

\textbf{Analiza glavnih komponent} (\textit{Principal component analysis}, PCA) je definirana kot ortogonalna linearna transformacija, ki transformira nabor podatkov v nov koordinatni sistem, in sicer tako, da komponenta z največjo varianco neke projekcije podatkov leži na prvi koordinati, druga največja na drugem mestu in tako naprej. Metoda je poenostavitev metode večdimenzionalnega skaliranja (\textit{Multi-dimensional scaling}, MDS).

Praktična uporaba opisanih metod z razultati na podatkih iz detektorja ATLAS je opisana v poglavju \ref{analiza-podatkov}.

\section{Metode za nadzorovano učenje}

Na področju strojnega učenja v grobem ločimo dva različna razreda metod: (1) \textbf{metode za nadzorovano učenje} in (2) \textbf{metode za nenadzorovano učenje}. Zadnji razred metod se ponavadi izvaja na podatkih, ki ne vsebujejo oznake $y^{(i)}$, torej parametra, ki nas zanima (npr. oznaka razreda). Zakonitosti iščemo torej samo v strukturi, ki jo oblikuje množica točk $x^{(i)}$ v faznem prostoru. Osnovni primer take metode je grozdenje, ki poskuša točke povezati s smiselne zaključene množice - grozde. Metoda grozdenja je lahko tudi učinkovita metoda generiranja novih značilk, pri čemer oznaka grozda služi kot dodatna značilka v nadzorovanem učnem problemu.

Metode za nadzorovano učenje (1) delujejo na podatkih z znanim parametrom, ki nas zanima ($y^{(i)}$). Ločimo regresijske in klasifikacijske metode. Pri \textbf{regresijskih metodah} je ponavadi parameter $y^{(i)} \in \mathbb{R}$. Cilj regresijskih metod je za neznano točko $x^{(i)}$ oceniti realno vrednost $y^{(i)}$. 

V tipičnem \textbf{klasifikacijskem problemu}, ki je tudi sicer predmet diplomskega dela, imamo množico vektorjev v prostoru $x^{(i)} \in \mathbb{R}^d$ (vektor značilk) in pripadajočih oznak $y^{(i)} \in {1, 2, 3, \ldots, N}$, kjer $N$ označuje število različnih razredov. Oznaka $y^{(i)}$ nam pove, v kateri razred spada vektor $x^{(i)}$. Cilj klasifikacijskega problema je predvideti oznake neznanih vektorjev, in sicer tako, da bo pri tem napaka najmanjša.

Za binarne klasifikacijske probleme velja $N = 2$. V takih primerih obstajata 2 razreda: pozitivni in negativni. S takšnim problemom se srečujemo tudi v tem diplomskem delu.


\section{Pregled klasifikacijskih metod}

Pri pregledu klasifikacijskih metod sem se omejil na naslednje metode: osnovne metode, ki prinašajo dodaten vpogled v podatke (logistična regresija), osnovne metode, ki jih je predlagal organizator (preprosto okno, naivni Bayesov klasifikator, pospešena odločitvena drevesa), zmagovalno metodo (globoke nevronske mreže) in na metodo, katere uporabo za dani problem sem želel v diplomskem delu sam raziskati - metodo podpornih vektorjev. 


\subsection{Logistična regresija}

Linearno regresijo je moč, ko imamo opravka z numeričnimi značilkami, uporabiti kot klasifikacijsko metodo. Pravzaprav lahko vsako regresijsko tehniko uporabimo za klasifikacijo. Prenos regresijske tehnike v klasifikacijsko domeno lahko naredimo s pomočjo preprostega trika, in sicer da pozitivno klasificiranim točkam pripišemo vrednost 1, negativno klasificiranim pa vrednost 0. Ko poskusimo s pomočjo naučenega modela klasificirati neznano točko, lahko vrednost, ki jo vrne linearna regresija, interpretiramo kot funkcijo članstva v določenem razredu. \cite{Witten2005}

Težava tega pristopa je, da se funkcija članstva ne nanaša na verjetnost pripadnosti vzorca določenemu razredu (saj je lahko večja od 1 ali manjša od 0), hkrati pa je binarna porazdelitev vrednosti točk daleč od normalne, ki jo zahteva metoda.

\comment{Povzeto po Andrew Ng, Stanford University on-line course, 10. 10. - 12. 12. 2011. Kako citirati?}
\comment{\url{http://cs229.stanford.edu/notes/cs229-notes1.pdf}}

\textit{Logistična regresija} je statistična metoda, ki nima težav iz zgornjega odstavka. Namesto, da bi poskusila vrednosti 0 in 1 aproksimirati neposredno, uporabi za ta namen transformacijsko funkcijo ($[0, 1] \rightarrow \mathbb{R}$). Katerokoli realno število pretvori na interval med 0 in 1.

Standardno logistično funkcijo zapišemo kot
\begin{equation}
	\label{en:standardna_log_funkcija}
	h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}},
\end{equation}
kjer se
\begin{equation}
	g(z) = \frac{1}{1 + e^{-z}}
\end{equation}
imenuje \textbf{logistična funkcija} ali {sigmoid}. Graf funkcije je prikazan na sliki \ref{sl:standardna_logisticna}.
$h(x)$ predstavlja linearno kombinacijo značilk $h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n$. Če privzamemo $x_0^{(i)} = 1$, lahko zapišemo $h(x) = \theta \cdot x$.

Interpretacija hipoteze je verjetnostna. Vrednost hipoteze je $P(y=1|x;\theta)$ - torej verjetnost, da je $y = 1$ pri danem $x$ in $\theta$.

Naloga učnega algoritma je določitev parametrov $\theta_i$. Najprej moramo definirati logistično minimizacijsko funkcijo $J(\theta)$. Ta funkcija je odvisna od razlike med hipotezo in dejansko vrednostjo točk. Pri linearni regresiji funkcijo zapišemo kot
\begin{equation}
	J(\theta) = \frac{1}{m} \sum^m_{i = 1} \frac{1}{2} \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2.
\end{equation}
Parametre $\theta$ izberemo tako, da je vsota kvadratnih členov kar najmanjša. Pri logistični regresiji sumand nadomestimo s \textit{cost} funkcijo $\text{Cost}(h_\theta(x), y)$. Zapišimo \textit{cost} funkcijo
\begin{equation}
	\text{Cost}(h_\theta(x), y) = \left\{
	\begin{array}{rl}
		-\log(h_\theta(x)) & \text{če je} \; y = 1 \\
		-\log(1 - h_\theta(x)) & \text{če je} \; y = 0
	\end{array}
	\right.
\end{equation}
Vemo, da je $y \in \{0, 1\}$. Zdaj lahko v eni sapi zapišemo $J(\theta)$
\begin{equation}
	J(\theta) = -\frac{1}{m} \left[ 
		\sum^m_{i = 1}
			y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
	\right].
	\label{en:j_theta}
\end{equation}
Naša učna naloga je poiskati tak $\theta$, da bo $J(\theta)$ najmanjši. Napoved za nov nepoznani $x$ izračunamo s pomočjo enačbe (\ref{en:standardna_log_funkcija}), ki nam predstavlja verjetnost, da dani primerek spada v pozitivni razred $P(y = 1 | x;\theta)$. $\theta$ bomo poiskali s pomočjo minimizacije $J(\theta)$, in sicer s pomočjo gradientnega spusta.

Ena pomembnih tehnik pri zagotavljanju uspešnega gradientnega spusta je skaliranje značilk. S tem se izognemo nepotrebnim oscilacijam, ki upočasnjujejo konvergenco. Oscilacije so posledica dejstva, da je velikostni red nekaterih značilk precej nižji od ostalih.

Osnovni algoritem gradientnega spusta je zelo preprost. Potrebno je izračunati parcialne odvode $\frac{\partial}{\partial x_i}J(\theta)$. Upoštevajoč enačbo (\ref{en:j_theta}) lahko zapišemo posamezni korak naše minimizacije
\begin{equation}
	\theta_j := \theta_j - \alpha \sum^m_{i = 1}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)},
	\label{en:log_reg_update}
\end{equation}  
pri čemer velja $j \in {1, 2, ...,  d}$, popravki pa se izvršijo simultano (torej ne vplivajo drug na drugega).

\iffalse
\subsection{Naivni Bayesov klasifikator}
\comment{Tole gre lahko mogoče ven, ker ne prinaša dodatne vrednosti, kot jo že logistična regresija.}

\comment{http://cs229.stanford.edu/notes/cs229-notes2.}

Pri logistični regresiji (podobno tudi pri perceptronu, glej spodaj) smo poskusili predvideti $p(y|x;\theta)$, pogojno porazdelitev $y$ pri danem $x$. Pri logistični regresiji smo verjetnost aproksimirali kar z logistično funkcijo $h_\theta(x) = g(\theta^Tx)$, kjer je $g$ sigmoidna funkcija. Pri klasifikacijskem problemu se tak algoritem nauči določiti mejo med različnimi razredi točk. Takim algoritmom pravimo \textbf{diskriminativni}.

Naivni Bayesov klasifikator je \textbf{generativni} algoritem. Ta se poskuša naučiti modelirati $p(x|y)$ (in $p(y)$). Povedno preprosteje, algoritem poskusi modelirati porazdelitev posamezne značilke v primeru, ko meritve spadajo v točno določen razred. Lepa ilustracija tega pristopa so histogrami, ki smo jih pridelali v raziskovalni analizi podatkov, je npr. na sliki \ref{sl:histogrami}.

Ko znamo izračunati $p(y)$ (imenovan tudi \textbf{class prior}) in $p(x|y)$, lahko uporabimo Bayesovo pravilo, da izpeljemo porazdelitev $p(y|x)$.
\begin{equation}
	p(y|x) = \frac{p(x|y)p(y)}{p(x)}.
\end{equation}
Imenovalec lahko izračunamo kot $p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$, v resnici pa ga ni potrebno nikoli izračunati, saj dejansko v klasifikacijskem algoritmu iščemo
\begin{equation}
	\arg \max_y p(y|x) = \arg \max_y \frac{p(x|y)p(y)}{p(x)} = \arg \max_y p(x|y)p(y).
\end{equation}
Privzemimo, da je porazdelitev $p(x|y)$ normalna. V $d$ dimenzijah takšno porazdelitev parametriziramo s pomočjo povprečja $\mu \in \mathbb{R}^d$ in kovariančne matrike $\Sigma \in \mathbb{R}^{d \times d}$, kjer je $\Sigma \geq 0$ simetrična in pozitivno semi definitna Porazdelitev zapišemo kot
\begin{equation}
	p(x;\mu,\Sigma) = {\cal N}(\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right),
\end{equation}
kjer $|\Sigma|$ označuje determinantno matrike $\Sigma$.

Model zapišimo tako, da ima $p(y)$ Bernoullijevo porazdelitev, $p(x|y)$ pa normalno:
\begin{eqnarray}
	p(y) &=& \phi^y(1-\phi)^{1-y} \\
	p(x|y = 0) &=& {\cal N}(\mu_0;\Sigma) \\
	p(x|y = 1) &=& {\cal N}(\mu_1;\Sigma)
\end{eqnarray}

\comment{Manjka!!!}

Naivni Bayesov klasifikator se uporablja predvsem v primerih, ko imamo opravka z binarnimi značilkami (npr. pri obravnavni teksta, najbolj značilna uporaba metode je pri klasifikaciji nezaželene elektronske pošte), uporaba v primerih, ko je značilka $x_i \in \{1, 2, ..., k_i\}$ preprosta, če uporabimo ustrezno porazdelitev za $p(x_i|y)$. Pred uporabo algoritma je zato potrebno zvezne značilke ustrezno diskretizirati.

Pri naivnem Bayesovem klasifikatorju privzamemo, da so značilke $x_i$ med seboj pogojno neodvisne. Pogojna neodvisnost pomeni, da bomo v primeru, ko bomo imeli opravka z npr. pozitivno klasificirano točko ($y = 1$), ki bo imela neki vrednosti značilk $x_1 = a_1$ in $x_2 = a_2$ privzeli, da naše vedenje o $x_1$ v ostalih primerih ne bo vplivalo na vedenje o $x_2$. To pomeni, da bo veljalo $p(x_1|y) = p(x_1|y, x_2)$. 

Sedaj lahko izračunamo verjetnost
\begin{equation}
	\begin{split}
	p(x_1,...,x_d|y) &= p(x_1|y)p(x_2|y, x_1)p(x_3|y,x_1,x_2) \cdots p(x_d|y,x_1,...x_{d-1}) \\
	& = p(x_1|y)p(x_2|y)p(x_3|y) \cdots p(x_d|y) \\
	& = \prod_{i=0}^{d} p(x_i|y).
	\end{split}
\end{equation}
Predpostavko pogojne neodvisnosti je sicer težko izpolniti, vendar končni algoritem dobro deluje na veliko različnih problemih.

Dobra lastnost algoritma je, da za relativno dobro delovanje potrebuje zelo malo primerov v učni podatkovni množici, kar pa v primeru obsežne zbirke podatkov, s katero imamo opravka v diplomskem delu, ni odločilna prednost.
\fi


\subsection{Metoda podpornih vektorjev}

Denimo, da imamo binarni klasifikacijski problem in da sta razreda točk v našem prostoru linearno separabilna. V večini takih primerov obstaja neskončno mnogo različnih hiperravnin, ki ločujejo med sabo oba razreda (glej sliko \ref{svmseparable}).

\begin{figure}[ht]
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_1.pdf}
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_2.pdf}
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_3.pdf}	
	
	\caption{Veliko različnih rešitev za hiperravnino, ki ločuje razreda primerkov.}
	\label{svmseparable}
\end{figure}

Metoda podpornih vektorjev (\textit{angl. Support Vector Machine} oz. SVM) se imenuje tudi klasifikator maksimalnega razmika. Prej opisani problem binarne klasifikacije s tem dobi enolično rešitev. Primer klasifikatorja z maksimalnim razmikom je ilustriran na sliki \ref{svmmaxclass}.

\comment{Pripravi sliko!}

\comment{Vključi komentar o več dimenzijah in uporabo kernelov.}

Hiperravnino, ki ločuje različna razreda primerkov označimo s $H$. Enačbo ravnine lahko zapišemo kot
\begin{equation}
  w^T x + b = 0,
\end{equation}
kjer je $w$ normalni vektor (v našem kontekstu ga bomo imenovali \textit{vektor uteži}), $b$ pa odmik ravnine v izhodišču.

Za enolično reprezentacijo hiperravnine je potrebno normalizirati faktorja $w$ in $b$. Taki reprezentaciji pravimo tudi kanonična reprezentancija.

\comment{Pride nekje do AMS 2,7.}
\comment{https://www.kaggle.com/c/higgs-boson/forums/t/10165/has-anyone-tried-the-support-vector-machine-for-this-problem}
\comment{Tu se splača igrat s kerneli.}




\iffalse
\subsection{Nevronske mreže}
\comment{Deep NN - zmagovalna metoda. Ansambli.}
\comment{NEVRONSKE MREŽE LAHKO GREDO VEN!}
\subsubsection{Perceptron}
Razpravo o nevronskih mrežah začenjamo pri algoritmu, imenovanem \textbf{perceptron}. V 60-ih letih prejšnjega stoletja so ta algoritem uvedli kot osnovni model delovanja nevronov v možganih. Kljub temu, da se razprava o perceptronu lepo navezuje na razpravo o logistični regresiji, je narava tega algoritma precej drugačna od ostalih. Predvsem je na podlagi rezultatov tega algoritma težko najti interpretacije, ki bi temeljile na verjetnosti ali oceni maksimalne podobnosti (maximum likelihood).

V primeru, da logistično regresijo spremenimo tako, da funkcijo $g(z)$ zapišemo v obliki Heavisidove funkcije:
\begin{equation}
	g(z) = \left\{
		\begin{array}{ll}
			1 & \text{če je} \; z \geq 0 \\
			0 & \text{če je} \; z \le 0
		\end{array}
	\right.
\end{equation}
Podobno kot v primeru logistične regresije določimo $h_\theta(x) = g(\theta^Tx)$. Pravilo za gradientni spust ostane enako definiciji v (\ref{en:log_reg_update}).

\subsubsection{Umetne nevronske mreže}

\subsubsection{Globoke nevronske mreže}

\subsubsection{Ansambli}

\comment{NEVRONSKE MREŽE LAHKO GREDO VEN!}
\fi

\subsection{Odločitvena drevesa}

\subsection{Ansambli klasifikacijskih algoritmov}

Odločitve ponavadi sprejemamo na podlagi enega modela, lahko pa kombiniramo več (ponavadi šibkejših) modelov in končno odločitev sprejmemo na podlagi njihovih rezultatov. Poznamo več vrst kombiniranja ansamblov modelov, in sicer: zlaganje (\textit{bagging}), pospeševanje (\textit{boosting}) in nalaganje (\textit{stacking}). \cite{Witten2005} Metode kombiniranja modelov so doživele razcvet šele v zadnjih dveh desetletjih, rezultati njihove uporabe pa so ponavadi presenetljivo dobri. Izmed vseh treh metod se je pospeševanje izkazalo za najbolj uspešno.

V literaturi pogosto zasledimo, da so metode kombiniranja modelov najbolj uspešne pri ansamblih odločitvenih dreves. Argumentacija pravi, da je generiranje odločitvenih dreves precej nestabilno. Na podobnih (vendar različnih podatkih) lahko dobimo modele, ki so si med seboj zelo različni. Zato vsak model nosi drugačno informacijo.

Zlaganje je poseben primer povprečenja rezultatov množice modelov. Algoritem je opisan na sliki \ref{sl:algo-bagging}. Pri zlaganju na prvotni učni množici podatkov naučimo $t$ različnih modelov, pri čemer učno podatkovno množico vedno generiramo z naključnimi izbirami vzorcev iz prvotne učne podatkovne množice. Novi meta-algoritem nam poda kot rezultat tisti razred izbranega vzorca, za katerega je \textit{glasovalo} več osnovnih algoritmov.

\begin{figure}[h!]
	\begin{algorithm}[H]
		\texttt{/* učenje množice modelov */}\\
		$n$ je število vrstic v učni množici podatkov\;
		$t$ je število modelov\;
		${\cal D}$ je učna množica podatkov\;
		\For{vsak $i$ od $1$ do $t$}{
			${\cal D}_i$ določi z izbiro $n$ naključnih dogodkov iz ${\cal D}$\;
			izvedi učni del metode na množici ${\cal D}_i$\;
			zapomni si model $M_i$
		}

		\texttt{/* klasifikacija */} \\
		\For{vsak $i$ od $1$ do $t$}{
			napovej razred $r_i$ s pomočjo modela $M_i$\;
		}
		vrni razred, ki je bil največkrat izbran
	\end{algorithm}
	\caption{Algoritem za zlaganje \textit{bagging}.}
	\label{sl:algo-bagging}	
\end{figure}

\subsection{Pospešeni gradientni spust}

\cite{Witten2005}



\comment{GBC iz scikit learn pride do $\sigma = 3,2+$.}
\comment{XGBoost - metoda, ki je manj potratna.}
\comment{http://higgsml.lal.in2p3.fr/software/multiboost/}
\comment{https://github.com/dmlc/xgboost}

\cite{chen2014}, \cite{chenG16}.

\section{Evalvacija klasifikacijskih metod}
\comment{Ne samo metrika, ampak celoten princip! Učna/testna množica, cross-validacija, ipd.}

Poleg metrike, ki je razložena v razdelku \ref{sc:ams} in je specifična problemu ločevanja signala in ozadja v primeru podatkov iz detektorja ATLAS, je v splošnem za razumevanje delovanja in izbiro klasifikacijskih metod priporočljivo uporabiti še nekaj metrik. Pregled metrik je povzet po \cite{wiki:precision_and_recall}.

\begin{itemize}
	\item \textbf{pravilno pozitivni (\textit{true positive}, TP)}, predstavlja število pravilno klasificiranih pozitivnih dogodkov (signal)
	\item \textbf{pravilno negativni (\textit{true negative}, TN)}, predstavlja število pravilno klasificiranih negativnih dogodkov (ozadje)
	\item \textbf{napačno pozitivni (\textit{false positive}, FP)}, predstavlja število napačno klasificiranih negativnih dogodkov (ozadje, klasificirano kot signal)
	\item \textbf{napačno negativni (\textit{false negative}, FN)}, predstavja število napačno klasificiranih pozitivnih dogodkov (signal, klasificiran kot ozadje)
	\item \textbf{natančnost (\textit{precision} ali \textit{positive prediction value}, PPV)} (pozitivna napovedna vrednost), definirana kot 
	\begin{equation}	
	PPV = \frac{TP}{TP + FP}
	\end{equation}
	\item \textbf{priklic (\textit{recall} ali \textit{true positive rate}, TNR)} (ali občutljivost), definiran kot
	\begin{equation}	
	TNR = \frac{TP}{TP + FN}
	\end{equation}
	\item \textbf{$F_1$ score} povezuje natančnost in priklic v geometrijskem povprečju. 
	\begin{equation}
	F_1 = 2 \cdot \frac{PPV \cdot TNR}{PPV + TTR} = \frac{2TP}{2TP + FP + FN}
	\end{equation}
	\item \textbf{točnost (\textit{accuracy}, ACC)}
	\begin{equation}
	ACC = \frac{TP + TN}{P + N}
	\end{equation}
\end{itemize}

\comment{Teorija je precej bolj podrobna, vendar se ne bi poglabljal.}

\comment{Mogoče bi moral razložiti tudi ROC krivuljo.}

% -----------------------------------------------------------------------------
% POGLAVJE: Podatki
% -----------------------------------------------------------------------------
\chapter{Podatki}
\label{analiza-podatkov}
Podatki, ki sem jih uporabil v diplomskem delu, so bili uporabljeni za izvedbo \textit{HiggsML Challenge}. Podatki predstavljajo dogodke, ki so bili simulirani na uradnem simulatorju polnega detektorja kolaboracija ATLAS\cite{Adam-Bourdarios14}. Podatki iz simulatorja imajo lastnosti, ki posnemajo statistične lastnosti dejanskih dogodkov - tako signala, kakor tudi ozadja.

Vzorec signala zajema dogodke, v katerih naj bi nastal Higgsov bozon (katrega masa je bila nastavljena na $m_H = 125\,\text{GeV}$). Vzorec, ki predstavlja ozadje, vsebuje dogodke, ki odražajo druge znane procese, ki lahko proizvajajo dogodke z vsaj enim elektronom ali mionom in hadronskim tau, ki posnemajo signal.

V simuliranih podatkov so predstavljeni samo trije procesi ozadja. Prvi prihaja od razpada $Z \rightarrow \tau^+\tau^-$ (masa z bozona je $m_Z = 91.2\,\text{GeV}$). Ta razpad proizvaja dogodke s topologijo, ki je zelo podobna tisti pri razpadu Higgsovega bozona. Druga množica dogodkov vsebuje par $t$ kvarkov, ki imata lahko med svojimi razpadnimi produkti lepton ali hadronski tau. Tretja množica vsebuje razpad $W$ bozona, kjer lahko nastaneta en elektron ali mion in hadronski tau. 

\section{Opis podatkov}
\label{ch:opis_podatkov}

V učni podatkovni množici je na voljo 250.000 dogodkov, od teh jih je 85.667 predstavljalo signal, 164.333 pa ozadje. Vsak dogodek je predstavljen z $d = 30$ značilkami, ki so dodatno opisane v \ref{sec:opis-znacilk}, in 3 metapodatki: oznako razreda ($s$ - signal ali $b$ - ozadje), utežjo $w_i$ in zaporedno številko dogodka. Raziskovalna analiza podatkov je predstavljena v \ref{sec:raziskovalna-analiza}.

Potrjevalna podatkovna množica vsebuje 100.000 dogodkov, ki pa niso opremljeni z oznako razreda ali utežjo $w_i$. 

\subsection{Opis značilk}
\label{sec:opis-znacilk}
Podatki zajemajo $d = 30$ značilk, katerih opis je povzet po \cite{Adam-Bourdarios14}. Stolpci s predpono PRI (ki označuje \textit{PRImitives}) predstavlja \textit{surove} podatke o trkih, kot jih izmeri detektor. Stolpci s predpono DER (ki označuje \textit{DERived}) predstavljajo vrednosti, izpeljane iz primitivnih lastnosti. Količine so določili sodelavci kolaboracije ATLAS z namenom, da bi pomagale definirati relevantna področja faznega prostora za klasifikacijske metode.

Stolpci brez predpone imajo posebno vlogo in se ne uporabljajo kot značilke:

\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item [EventId] 	Zaporedna številka dogodka.
	\item [Weight]  	Utež dogodka $w_i$, kakor je opisana v podpoglavju \ref{utez}.
	\item [Label] 		Oznaka dogodka (niz) $y_i \in \{\text{s}, \text{b}\}$ (s za signal, b za ozadje).	
\end{description}
\end{changemargin}

Stolpci s predpono \textbf{PRI} označujejo \textit{surove} podatke, ki jih detektor izmeri pri trku:
\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item[PRI\_tau\_pt] Prečni (transverse???) moment $\sqrt{p_x^2 + p_y^2}$ hadronskega tau.
	\item[PRI\_tau\_eta] Psevdorapidnost $\eta$ hadronskega tau.
	\item[PRI\_tau\_phi] Azimutalni kot $\phi$ hadronskega tau.
	
	\item[PRI\_lep\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ leptona (elektrona ali miona).
	\item[PRI\_lep\_eta] Psevdorapidnost $\eta$ leptona.
	\item[PRI\_lep\_phi] Azimutalni kot $\phi$ leptona.
	
	\item[PRI\_met] Manjkajoča prečna energija $\vec{E}_T^{\text{miss}}$.
	\item[PRI\_met\_phi] Azimutalni kot $\phi$ manjkajoče prečne energije.
	
	\item[PRI\_met\_sumet] Celotna prečna energija v detektorju.
	
	\item[PRI\_jet\_num] Število vseh žarkov ($\in {1, 2, 3}$), vednosti, večje od $3$ so bile zamenjane s 3
	
	\item[PRI\_jet\_leading\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ glavnega žarka, t. j. žarka z največjo prečno energijo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	\item[PRI\_jet\_leading\_eta] Psevdorapidnost $\eta$ glavnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	\item[PRI\_jet\_leading\_phi] Azimutalni kot $\phi$ glavnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	
	\item[PRI\_jet\_subleading\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ prvega pomožnega žarka, t. j. žarka z drugo največjo prečno energijo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	\item[PRI\_jet\_subleading\_eta] Psevdorapidnost $\eta$ prvega pomožnega žarka, t. j. žarka z drugo največjo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	\item[PRI\_jet\_subleading\_phi] Azimutalni kot $\phi$ prvega pomožnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	
	\item[PRI\_jet\_all\_pt] Skalarna vsota prečnih momentov vseh žarkov pri dogodkih.
\end{description}
\end{changemargin}

Stolpci s predpono \textbf{DER} označujejo iz surovih podatkov izpeljane količine:
\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item[DER\_mass\_MMC] Ocenjena masa kandidata za Higgsov bozon $m_H$, pridobljena s pomočjo (\comment{MC?}) verjetnostne integracije po faznem prostoru (vrednost ni definirana v primeru, da topologija dogodka preveč odstopa od pričakovane)
	\item[DER\_mass\_transverse\_met\_lep] Prečna masa med manjkajočo prečno energijo in leptonom.
	\item[DER\_mass\_vis] Invarianta masa hadronskega $\tau$ in leptona.
	\item[DER\_pt\_h] Modulus ... \comment{uf, preveri!}
	\item[DER\_deltaeta\_jet\_jet] Absolutna vrednost psevdorapidnostne separacije med obema žarkoma (vrednost ni definirana v primeru $PRI\_jet\_num \le 1$).
	\item[DER\_mass\_jet\_jet] Invariantna masa obeh žarkov (vrednost ni definirana pri $PRI\_jet\_num \le 1$).
	\item[DER\_prodeta\_jet\_jest] Produkt psevdorapidnosti obeh žarkov (vrednost ni definirana pri $PRI\_jet\_num \le 1$).
	
\end{description}
\end{changemargin}

\comment{Vse izpeljane vrednosti so definirane v dodatku! Referenciranje na ustrezne enačbe!}

\section{Rezultati raziskovalne analize podatkov}
\label{sec:raziskovalna-analiza}

V raziskovalni analizi podatkov sem najprej pregledal \textbf{porazdelitev posameznih značilk, uteži in oznak skozi celotno množico}. Ugotovil sem, da so vse količine enakomerno porazdeljene, kar nam zagotavlja, da poljubno rezanje množice podatkov (npr. na učno in testno množico) ne bo bistveno vplivalo na končne rezultate.

V drugem koraku sem si ogledal \textbf{porazdelitev uteži} $w^{(i)}$ pri obeh razredih. Rezultat je predstavljen na sliki \ref{sl:weight}. Povprečna utež pri signalu je $w = 0.0080 \pm 0.0082$, pri ozadju pa $w = 2.501 \pm 1.794$. Razlika v utežeh vpliva na optimizacijo algoritmov, kot je razloženo v poglavju \ref{ch:razvoj_lastne_metode}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=10cm]{exploratory/ea_weight.pdf} 
	\caption{Porazdelitev uteži $w^{(i)}$ v učni podatkovni množici.}
	\label{sl:weight}		
\end{figure}

Množica podatkov vsebuje veliko manjkajočih vrednosti. Gre za značilke, ki jih pri določenih topologijah ni bilo mogoče izmeriti ali izračunati. Rezultati so predstavljeni v tabeli \ref{tb:manjkajoce_vrednosti}. V celotni množici več kot $70\%$ točk nima vseh podatkov. Strategije pristopov pri reševanju težav, ki nastanejo zaradi manjkajočih vrednosti, so opisane v poglavju \ref{ch:razvoj_lastne_metode}.

\begin{table}[ht]
	\centering
	\begin{tabular}{lrr}
		\hline
		\textbf{Značilka} &         \textbf{Signal} &         \textbf{ozadje} \\
		\hline
		DER\_mass\_MMC            &  3.3\%  &  21.5\% \\
		DER\_deltaeta\_jet\_jet   &  62.1\% &  75.6\% \\
		DER\_mass\_jet\_jet       &  62.1\% &  75.6\% \\
		DER\_prodeta\_jet\_jet    &  62.1\% &  75.6\% \\
		DER\_lep\_eta\_centrality &  62.1\% &  75.6\% \\
		PRI\_jet\_leading\_pt     &  29.8\% &  45.3\% \\
		PRI\_jet\_leading\_eta    &  29.8\% &  45.3\% \\
		PRI\_jet\_leading\_phi    &  29.8\% &  45.3\% \\
		PRI\_jet\_subleading\_pt  &  62.1\% &  75.6\% \\
		PRI\_jet\_subleading\_eta &  62.1\% &  75.6\% \\
		PRI\_jet\_subleading\_phi &  62.1\% &  75.6\% \\
	\end{tabular}
	\caption{Nedefinirane vrednosti v dogodkih, ki označujejo signal in ozadje, normalizirane na število dogodkov v posameznem razredu.}
	\label{tb:manjkajoce_vrednosti}
\end{table}

Pregled povzetka značilk nam pove, da so značilke, ki se med razredi najbolj razlikujejo \comment{naštej}. Povzetek vseh značilk je prikazan v tabeli \ref{tb:5figuresum}. Natančnejši pogled posamičnih značilk je možen s pomočjo histogramov. Nekaj predstavnikov, pri katerih se porazdelitve med razredi najbolj razlikujejo, je predstaviljenih na sliki \ref{sl:histogrami}, celoten pregled izmerjenih in izračunanih značilk pa je na voljo na slikah \ref{sl:histogram_izmerjene} in \ref{sl:histogram_izpeljane}.

\begin{figure}[ht]
	\includegraphics[width=7.7cm]{exploratory/ea_hist_der_mass_mmc.pdf}
	\includegraphics[width=7.8cm]{exploratory/ea_hist_der_mass_transverse_met_lep.pdf}	
	\includegraphics[width=7.8cm]{exploratory/ea_hist_der_deltaeta_jet_jet.pdf}		
	\includegraphics[width=7.8cm]{exploratory/ea_hist_pri_jet_subleading_eta.pdf}	
	\caption{Nekaj tipičnih histogramov za značilke, katerih porazdelitev se med razredoma razlikuje. Rumena barva prikazuje signal, zelena pa ozadje.}
	\label{sl:histogrami}			
\end{figure}

Korelacijo dveh značilk in njun potencial pri ločevanju signala od ozadja si lahko ogledamo na razpršenih grafih. Trije primeri so prikazani v dodatku, in sicer dva, ki ponazarjata močno korelirane značilke (glej sliki \ref{sl:scatter_corr} in \ref{sl:scatter_corr2}), in en, ki ponazarja nekorelirane značilke (slika \ref{sl:scatter_noncorr}).

Bolj popolna informacija o korelaciji posameznih značilk je prikazana na sliki \ref{sl:corr_clust_matrix}. Podobno obarvane vrstice oz. stolpci (ki so velikosti $d$) na sliki prikazujejo značilke, ki so si med seboj zelo podobne in podobno korelirajo z drugimi značilkami. Vsaka vrstica oz. stolpec ponazarja vektor korelacij posamezne značilke z vsemi ostalimi.

Hierarhično grozdenje temelji na evklidski razdalji med vektorji korelacij, njegov rezultat pa je ponazorjen s skicama nad in levo od matrike korelacij. Podobnost dveh značilk lahko bralec oceni z odmikom razcepa, ki ponazarja posamični grozd.

S slike je razvidno, da obstaja en močno koreliran grozd značilk (zgoraj levo), ki zajema značilke: PRI\_met, PRI\_jet\_subleading\_pt, PRI\_jet\_num, PRI\_jet\_all\_pt, DER\_sum\_pt, PRI\_met\_sumet, DER\_pt\_h in PRI\_jet\_leading\_pt. Nobena izmed omenjenih značilk ne vsebuje veliko novih informacij, ki bi lahko pripomogle k izboljšanju klasifikacijskih metod, kakor katerakoli druga iz tega grozda. Podobnih grozdov ali vsaj parov (trojic) značilk lahko iz slike razberemo še precej.

Slika korelacij nam že sama po sebi da zelo informativen vpogled v podatke, pri modeliranju pa bi nam lahko služila kot orodje za izbiro značilk. V našem primeru se sicer zdi zmanjševanje značilk nepotrebno, saj razpolagamo z nekaj velikostnih redov večjim naborom podatkov, kakor pa je nabor značilk. Zaradi tega ne tvegamo pojava prenasičenja (\textit{overfitting}), pri čemer bi zaradi prilagajanja posameznim vzorcem iz učne množice podatkov izgubili na splošnosti modela.

\begin{figure}[ht]
	\centering
	\includegraphics[width=12cm]{exploratory/ea_corr_clustermap.pdf}
	\caption{Koreliranost značilk v simuliranih (merjenih) podatkih. Značilke so urejene urejenost na podlagi hierarhičnega grozdenja.}
	\label{sl:corr_clust_matrix}
\end{figure}

Na zadnji sliki (\ref{sl:pca}) je prikazanih nekaj vizualizacij na podlagi analize glavnih komponent (PCA). Namen te analize je preveriti potencial za linearno separabilnost vzorcev, ki pripadajo različnima razredoma. 


Na vseh treh grafih je moč s prostim očesom razločiti območja, ki pripadajo predvsem signalu (rdeča) oz. ozadju (zelena).

\begin{figure}[h]	
	\includegraphics[width=5.2cm]{exploratory/ea_pca_2d.pdf}
	\includegraphics[width=5.2cm]{exploratory/ea_pca_2d_1_3.pdf}
	\includegraphics[width=5.2cm]{exploratory/ea_pca_2d_2_3.pdf}		
	
	\caption{Analiza glavnih komponent. Razpršeni grafi 1. in 2., 1. in 3. ter 2. in 3. komponente.}
	\label{sl:pca}
\end{figure}

Nekaj slik iz podrobnejše raziskovalne analize podatkov je na voljo v dodatku \ref{ch:dodatek_raziskovalna}.

	
% -----------------------------------------------------------------------------
% POGLAVJE: Rezultati osnovnih metod
% -----------------------------------------------------------------------------
\chapter{Rezultati osnovnih metod}

Osnovne metode so bile predlagane in vsaj delno implementirane že v spremljevalnem gradivu HiggsML izziva. Vse teste in dopolnitve sem sicer naredil v programskem jeziku Python z uporabo paketa \texttt{scikit-learn}\cite{scikit-learn}. Za pospešena odločitvena drevesa sem poleg implementacije v \texttt{scikit-learn} uporabil še nagrajeni paket \texttt{XGBoost}\cite{chen2014}.

	
\section{Preprosto okno}

Metoda preprostega okna temelji na izbiri preprostega okna s pomočjo ene same spremenljivke (ki naj bi vsebovalo signal)\footnote{\url{https://higgsml.lal.in2p3.fr/software/simplest-python-kit/}}. Predlagana implementacija temelji na oceni mase Higgsovega bozona (spremenljivka DER\_mass\_MMC). Dogodke, ki ustrezajo $m_H = 125\,\text{GeV} \pm 22\,\text{GeV}$, pri čemer je $22\,\text{GeV}$ predlog praga, metoda klasificira kot pozitivne (signal).

\begin{table}[ht]
	\centering
	\begin{tabular}{rl}
		\hline
		\textbf{Mera} & \textbf{Rezultat} \\
		\hline
		pravilno pozitivni & 23,2\%\\
		pravilno negativni & 50,8\% \\
		napačno pozitivni & 14,8\% \\
		napačno negativni & 11,2\% \\
		natančnost & 0.611 \\
		priklic & 0.673 \\
		točnost & 0.740 \\
		ocena $F_1$ & 0.641 \\
		ocena $AMS_2$ & 1.514 \\
		ocena $AMS_{2(test)}$ & 1.535 		
	\end{tabular}
	\caption{Rezultati predlagane metode preprostega okna na podlagi $m_H$.}
	\label{tb:preprosto_okno}
\end{table}

Preprosta metoda doseže na potrjevalni množici podatkov oceno $AMS_{2(test)} = 1.535$, to oceno je moč razbrati tudi iz zadnje vrstice tabele \ref{tb:preprosto_okno}. Naključna rešitev, ki služi kot osnovna metoda, da rezultat $AMS_2{(test)} = 0.586$.

Pristop sicer kliče po optimizaciji in izboljšanju metode. V prvem koraku lahko na podlagi mere ($AMS_2$) izboljšamo vrednost praga. Glede na porazdelitev značilke DER\_mass\_MMC glede na oba razreda pa lahko sklepamo, da bi lahko optimizirali tudi sredino okna. Optimizacija po pragu je prikazana na sliki \ref{sl:simple_optimization_threshold}.

\begin{figure}[h]
	\centering	
	\includegraphics[width=7.6cm]{methods/simple/sw_amslearn.pdf}
	\includegraphics[width=7.6cm]{methods/simple/sw_amslearn_detail.pdf}		
	
	\caption{Optimizacija praga pri metodi preprostega okna.}
	\label{sl:simple_optimization_threshold}
\end{figure}

Izbira praga pri ($\pm 22\,\text{GeV}$) ni bila optimalna. Metoda prag postavlja približno na $\pm 31\,\text{GeV}$, pri čemer velja komentirati, da je na območju med $\pm 38\,\text{GeV}$ in $\pm 30\,\text{GeV}$ napaka pri izračunu $AMS_2$ velika v primerjavi z dejansko obliko krivulje, zato lahko velikost praga zgolj približno ocenimo, in sicer na $\pm 31\,\text{GeV}$. Rezultati so prikazani v tabeli \ref{tb:preprosto_okno_optimized}

\begin{table}[ht]
	\centering
	\begin{tabular}{rl}
		\hline
		\textbf{Mera} & \textbf{Rezultat} \\
		\hline
		pravilno pozitivni & 28,2\%\\
		pravilno negativni & 43,0\% \\
		napačno pozitivni & 22,5\% \\
		napačno negativni & 6,3\% \\
		natančnost & 0.556 \\
		priklic & 0.819 \\
		točnost & 0.712 \\
		ocena $F_1$ & 0.662 \\
		ocena $AMS_2$ & 1.573 \\
		ocena $AMS_{2(test)}$ & 1.583 		
	\end{tabular}
	\caption{Rezultati metode preprostega okna na podlagi $m_H$ z optimiziranim pragom za mero $AMS_2$.}
	\label{tb:preprosto_okno_optimized}
\end{table}

Optimizacija po dveh parametrih (prah in $m_H$) je prikazan na sliki \ref{sl:simple_optimization_2d}. Po pregledu faznega prostora sem izluščil območje, kjer je $AMS_2$ najvišji. Določil sem $m_H = 127,25\,\text{GeV} \pm 32.25\,\text{GeV}$. Na levem grafu je celostni pregled zanimivega faznega prostora, na desnem pa povečan del z najvišjimi vrednostmi $AMS_2$. Na $x$ osi je prikazan prag, na $y$ osi je prikazana $m_H$. $AMS_2$ je predstavljen ob plastnicah. Modra barva predstavlja plastnice z nižjo, rdeča pa plastnice z višjo vrednostjo $AMS_2$.

\begin{figure}[h]
	\centering	
	\includegraphics[width=7.6cm]{methods/simple/sw_amslearn_2d_contour.pdf}
	\includegraphics[width=7.6cm]{methods/simple/sw_amslearn_2d_contour_detail.pdf}		
	
	\caption{Optimizacija praga in $m_H$ pri metodi preprostega okna.}
	\label{sl:simple_optimization_2d}
\end{figure}

Rezultati tako optimizirane metode so prikazani v tabeli \ref{tb:preprosto_okno_2d_optimized}.

\begin{table}[ht]
	\centering
	\begin{tabular}{rl}
		\hline
		\textbf{Mera} & \textbf{Rezultat} \\
		\hline
		pravilno pozitivni & 28,4\%\\
		pravilno negativni & 43,2\% \\
		napačno pozitivni & 22,3\% \\
		napačno negativni & 6,1\% \\
		natančnost & 0.560 \\
		priklic & 0.824 \\
		točnost & 0.716 \\
		ocena $F_1$ & 0.667 \\
		ocena $AMS_2$ & 1.579 \\
		ocena $AMS_{2(test)}$ & 1.587 		
	\end{tabular}
	\caption{Rezultati metode preprostega okna na podlagi $m_H$ z optimiziranim pragom in $m_H$ za mero $AMS_2$.}
	\label{tb:preprosto_okno_2d_optimized}
\end{table}

\section{Logistična regresija}
Uporaba logistične regresije na problemu ločevanja signala od ozadja nam bo dala prvi vpogled v \textit{domet} linearne ekspresivnosti značilk. Rezultati osnovne metode so prikazani v tabeli \ref{tb:logisticna}. Metodo lahko izkoristimo predvsem zato, da dobimo dodatni vpogled v podatke. Pri raziskovalni analizi podatkov smo imeli vpogled v modeliranje po eni ali dveh značilkah, logistična regresija pa upošteva vse. 

S slike \ref{sl:logistic_weights} lahko razberemo, da so najbolj dominantne značilke pri oblikovanju linearnega modela poleg števila curkov (PRI\_jet\_num) predvsem izpeljane vrednosti: DER\_deltar\_tau\_lep, DER\_pt\_ratio\_lep\_tau, DER\_lep\_eta\_centrality in DER\_deltaeta\_jet\_jet.

\begin{figure}[h]
	\centering	
	\includegraphics[width=15.6cm]{methods/logistic/lr_weights.pdf}
	
	\caption{Absolutne vrednosti uteži pri logistični regresiji (logaritemska skala na osi $y$).}
	\label{sl:logistic_weights}
\end{figure}

Uspešnost metode je primerljiva z uspešnostjo naivnega Bayesovega klasifikatorja ($AMS_2 \approx 2$). Z veliko verjetnostjo lahko zato trdimo, da z linearno kombinacijo značilk, s katerimi razpolagamo, ne glede na izbrano klasifikacijsko metodo, lahko dosežemo približno podobno dober rezultat.

Napovedno vrednost klasifikacijskega algoritma bomo torej znatno lahko izboljšali zgolj z vpeljavo novih značilk, predvsem takih, ki v sistem vnašajo nelinearne relacije. Podoben rezultat lahko npr. pričakujemo tudi od metode podpornih vektorjev (brez uporabe jeder ali razširjenega nabora značilk).

\begin{table}[h!]
	\centering
	\begin{tabular}{rl}
		\hline
		\textbf{Mera} & \textbf{Rezultat} \\
		\hline
		pravilno pozitivni & 18,4\%\\
		pravilno negativni & 56,4\% \\
		napačno pozitivni & 9,1\% \\
		napačno negativni & 16,1\% \\
		natančnost & 0.668 \\
		priklic & 0.535 \\
		točnost & 0.749 \\
		ocena $F_1$ & 0.594 \\
		ocena $AMS_2$ & 2.015 \\
		ocena $AMS_{2(test)}$ & 1.998 		
	\end{tabular}
	\caption{Rezultati neoptimizirane logistične regresije.}
	\label{tb:logisticna}
\end{table}

\comment{Zanimivo bi bilo pogledati, kako daleč pridemo samo s PRImarnimi značilkami.}


\iffalse
\section{Naivni Bayesov klasifikator}
\footnote{\url{https://higgsml.lal.in2p3.fr/software/starting-kit/}}
\fi


\section{Pospešena odločitvena drevesa}

\footnote{\url{https://github.com/dmlc/xgboost/}}

	
% -----------------------------------------------------------------------------
% POGLAVJE: Izboljševanje metod in razvoj lastne metode
% -----------------------------------------------------------------------------
\chapter{Razvoj lastne metode}
\label{ch:razvoj_lastne_metode}

Metodi, ki sta se pri reševanju danega problema odrezali najbolje, sta bili metodo globokih nevronskih mrež in pa pospešena odločitvena drevesa. Kljub temu, da je bil najboljši rezultat $AMS$ dosežen s pomočjo globokih nevronskih mrež, je izbrana metoda pri ločevanju signala od ozadja na projektu ATLAS metoda, ki temelji na pospošenih odločitvenih drevesih. Značilnost obeh metod je, da v sistem vpeljeta nelinearnost. 

Nekaj poročil je bilo o uporabi metode podpornih vektorjev (SVM)\footnote{\url{https://www.kaggle.com/c/higgs-boson/forums/t/10165/}}, vendar so tekmovalci poročali o precej nižjem rezultati, kakor je bil npr. osnovni rezultat metode pospešenih odločitvenih dreves.

Nelinearnost lahko v SVM modele pripeljemo s pomočjo jeder (\textit{kernel}) ali neposredno - s pomočjo generiranjem večjega nabora novih značilk, ki temeljijo na izvirnih značilkah. V grobem lahko zapišemo $x_j = f(x_{k_1}, x_{k_2}, ..., x_{k_l})$, pri čemerj je $l$ celo število in indeks izpeljane značilke $j > d$. Ponavadi velja $l \in \{1, 2, 3\}$. Funkcija $f$ je lahko poljubna. Pri vrednosti $l = 1$ lahko vpeljemo kvadrat, kub, logaritem, eksponent ali kakšno drugo podobno funkcijo. Pri večjem naboru se osredotočimo predvsem na produkte, vsote, diference, eksponente značilk v kombinaciji s prej omenjenimi funkcijami. 

Pri generiranju novih značilk šteje iznajdljivost in inovativnost. Zanimiv pristop je npr. uporaba metode grozdenja (npr. KMeans) in uporaba zaporedne številke grozda kot nove značilke.

Lepa lastnost metode SVM je, da se dobro obnese v primerih, ko imamo veliko število značilk. Še več, SVM zna najti \textit{iglo v kopici sena}. Zna torej v ogromnem naboru značilk poiskati tisto, ki najbolje odraža dinamiko modela. S pomočjo poskusov in napak je moč ob sistematičnem pregledu sicer neomejenega prostora izpeljanih značlik najti tiste, ki največ prispevajo k izboljšanju modela. Še ena lepa prednost metode SVM pred npr. nevronskimi mrežami je, da ne gre za \textit{črno škatlo}, pač pa je moč rezultate in končni SVM model interpretirati.

Glede na to, da globoke nevronske mreže nimajo višje izrazne vrednosti kot SVM obogaten z gornjimi izpeljanimi značilkami, je cilj razvoja algoritma, da se približa rezultatom pospešnih odločitvenih dreves in da preseže rezultate, o katerih so poročali drugi tekmovalci ($AMS > 2.7$).


\section{Osnovna implementacija}
Za implementacijo sem uporabil programski jezik Python. Za nalaganje in čiščenje podatkov sem uporabil knjižnico \texttt{Pandas}, implementacijo SVM in metode za skaliranje sem vzel iz paketa \texttt{scikit-learn}\cite{scikit-learn}. Vsa koda, razvita in preizkušena v diplomskem delu, je dosegljiva na sistemu GitHub\footnote{\url{https://github.com/klemenkenda/HiggsML}}.

Od vseh preizkušenih metod je prav SVM najbolj občutljiv na velikost značilk. Algoritem za dobro delovanje potrebuje značilke, ki so podobno skalirane. V našem primeru smo vse značilke linearno transformirali tako, da imajo povprečno vrednost $0$ in standardno deviacijo $1$.


\section{Generiranje in izbira novih značilk}
Med rezultati učnega procesa linearnega SVM je tudi matrika koeficientov, ki izražajo pomembnost posameznih značilk. Podobno funkcionalnost smo dosegli tudi pri logistični regresiji (glej sliko \ref{sl:logistic_weights}).

Število različnih značilk, je lahko ogromno. Najprej sem generiral značilke, ki so povedale, ali je določena vrednost bila izmerjena oz. izračunana (to je informacija, ki se je s strategijo nadomeščanja manjkajočih vrednosti izgubila). Takih značilk je 10. Nato sem vsako izmed značilk transformiral z izbrano nelinearno funkcijo ($x^2$, $x^3$, $e^{x}$, $\log(x)$, $\sqrt{x}$). Vsak tak poizkus je dodal 30 novih značilk - skupaj 150.

V naslednjem koraku sem vpeljal nelinearne funkcije dveh značilk $x_i$ in $x_j$. Za vsako smiselno transformacijsko funkcijo lahko generiramo $d^2 = 900$ novih značilk. Ker gre večinoma za simetrične značilke, je kombinacij $d(d-1)/2 = 435$. Uporabil sem naslednje transformacijske funkcije: $x_ix_j$, $x_i^2 + x_j^2$, $e^{x_i^2 + x_j^2}$, $\sqrt{x_i^2 + x_j^2}$, $(1 + x_ix_j)^2$. V vseh eksperimentih sem preizkusil skupno več kot 2000 značilk.

Ker gre za nepregledno množico značilk je praktično nemogoče v omejenem času preveriti vse in izbrati optimalno kombinacijo značilk in parametrov. V prvi fazi sem zato modelu dodajal omejeno število značilk in izbiral tiste, ki jih je SVM metoda spoznala za najbolj relevantne. Nekateri od vmesnih rezultatov so prikazani v tabeli \ref{tb:svm}.

\begin{figure}[h]
	\centering	
	\includegraphics[width=15.6cm]{methods/svm/svm_weights_2xy.pdf}
	
	\caption{SVM uteži generiranih značilk v primeru $x_ix_j$.}
	\label{sl:svm_xy_weights}
\end{figure}

Primer relevantnosti posameznih značilk v modelu SVM je prikazan na sliki \ref{sl:svm_xy_weights}. Izbral sem nekaj najbolj relevantnih značilk, mejo pa postavil tam, kjer se je število značilk potem znatno zgostilo. V primeru s prejšnje slike sem mejo postavil pri uteži SVM 0.9 in tako izbral najpomebnejših 6 značilk.

Izbrane transformiranke ene značilke:
\begin{itemize}
	\item \textbf{Manjkajoče vrednosti}: Manjkajoči DER\_mass\_MMC
	\item \textbf{Kvadratna transformacijska funkcija $x^2$}: DER\_mass\_MMC, DER\_mass\_vis, \\ 
		DER\_deltar\_tau\_lep, DER\_pt\_tot, PRI\_met
	\item \textbf{Kubična transformacijska funkcija $x^3$}: /
	\item \textbf{Eksponentna transformacijska funkcija: $e^x$}: DER\_sum\_pt, PRI\_met\_sumet
	\item \textbf{Korenska transofrmacijska funkcija: $\sqrt{x}$}: DER\_mass\_MMC, DER\_mass\_vis, \\	DER\_deltar\_tau\_lep, DER\_pt\_ratio\_lep\_tau, PRI\_met
	\item \textbf{Logaritemska transformacijska funkcija $\log(x)$}: /
\end{itemize}

Kubične in logaritemske transformiranke so znatno poslabšale rezultat klasifikatorja, zato sem jih izpustil.

Izbrane transformiranke dveh značilk:
\begin{itemize}
	\item \textbf{Zmnožek $x_ix_j$}: DER\_mass\_jet\_jet-DER\_mass\_MMC, DER\_mass\_jet\_jet- \\ DER\_mass\_vis, DER\_lep\_eta\_centrality-DER\_mass\_transverse\_met\_lep, \\ DER\_lep\_eta\_centrality-DER\_pt\_ratio\_lep\_tau, PRI\_tau\_phi-PRI\_tau\_pt, \\ PRI\_jet\_subleading\_eta-PRI\_tau\_pt
	
	\item \textbf{Vsota kvadratov $x_i^2 + x_j^2$}: DER\_mass\_jet\_jet-DER\_mass\_MMC, DER\_mass\_jet\_jet-DER\_mass\_vis, DER\_lep\_eta\_centrality-DER\_mass\_transverse\_met\_lep, \\
	DER\_lep\_eta\_centrality-DER\_pt\_ratio\_lep\_tau, PRI\_tau\_phi-PRI\_tau\_pt, \\ PRI\_jet\_subleading\_eta-PRI\_tau\_pt
	
	\item \textbf{Eksponent vsote kvadratov $e^{x_i^2 + x_j^2}$}: DER\_pt\_h-DER\_mass\_vis, \\ DER\_deltar\_tau\_lep-DER\_mass\_MMC, DER\_deltar\_tau\_lep-DER\_mass\_vis, \\ PRI\_tau\_pt-DER\_mass\_MMC, PRI\_tau\_pt-DER\_sum\_pt, PRI\_lep\_pt- \\ DER\_mass\_MMC, PRI\_lep\_pt-DER\_sum\_pt
	
	\item \textbf{Koren vsote kvadratov $\sqrt{x_i^2 + x_j^2}$}:  DER\_mass\_jet\_jet-DER\_mass\_MMC, \\ DER\_mass\_jet\_jet-DER\_mass\_vis, DER\_lep\_eta\_centrality- \\ DER\_mass\_transverse\_met\_lep,
	DER\_lep\_eta\_centrality-DER\_pt\_ratio\_lep\_tau, \\ PRI\_tau\_phi-PRI\_tau\_pt, PRI\_jet\_subleading\_eta-PRI\_tau\_pt
	
	\item \textbf{Kvadratni polinom $(1 + x_ix_j)^2$}: DER\_pt\_h-DER\_mass\_vis, \\
	DER\_deltar\_tau\_lep-DER\_mass\_MMC, DER\_deltar\_tau\_lep-DER\_mass\_vis,  \\ PRI\_tau\_pt-DER\_mass\_MMC,
	PRI\_tau\_pt-DER\_sum\_pt, PRI\_lep\_pt- \\ DER\_mass\_MMC, PRI\_lep\_pt-DER\_sum\_pt
	
\end{itemize}

Izbrani pari značilk so pri vsoti kvadratov in korenu vsote kvadratov enaki. To je mogoče indikator, da koren ne prinese dodatne informacije v klasifikacijski algoritem.

\comment{Featurji iz nelabeliranih podatkov - KMeans!}

\section{Jedra}

Preskusil sem naslednja standardna jedra SVM: (1) linearnega, (2) polinomskega in (3) RBF (Gaussovskega). Rezultati so zabeleženi v tabeli \ref{tb:svm},


\section{Ansambli}



\section{Rezultati}

\begin{table}[h!]
	\centering
	\begin{tabular}{r|cccc|cccc|c}		
		\textbf{Jedro \hfill \break (značilke)} & 
			\rotatebox[origin=l]{90}{pravilno pozitivni} & 
			\rotatebox[origin=l]{90}{pravilno negativni} & 
			\rotatebox[origin=l]{90}{napačno pozitivni} & 
			\rotatebox[origin=l]{90}{napačno negativni} &
			\rotatebox[origin=l]{90}{natančnost} & 
			\rotatebox[origin=l]{90}{priklic} & 
			\rotatebox[origin=l]{90}{točnost} & 
			\rotatebox[origin=l]{90}{ocena $F_1$} & 
			\rotatebox[origin=l]{90}{ocena $AMS_2$} \\
		\hline	
		RBF (1) & 23,7\% & 58,1\% & 7,4\% & 10,7\% &
			0,761 & 0,689 & 0,819 & 0,724 &
			2,674 \\
		LIN (1) & 18,9\% & 56,0\% & 9,5\% & 15,6\% &
			0,665 & 0,548 & 0,749 & 0,600 &
			1,999 \\
		POLY (1) & 19,1\% & 59,7\% & 5,9\% & 15,3\% &
			0,764 & 0,555 & 0,787 & 0,643 &
			2,345 \\
		POLY, $c=1$ (1) & 24,1\% & 57,6\% & 8,0\% & 10,4\% &
			0,752 & 0,699 & 0,817 & 0,724 &
			2,632 \\
		POLY$^2$, $c=1$ (1) &  23,8\% & 58,0\% & 7,5\% & 10,6\% &
			0,759 & 0,691 & 0,818 & 0,723 & 
			2,633 \\		
		POLY$^4$, $c=1$ (1) & 24,0\% & 57,0\% & 8,5\% & 10,4\% &
			0,738 & 0,698 & 0,811 & 0,718 & 
			2,570 \\
		RBF (2) & 23,8\% & 58,2\% & 7,4\% & 10,6\% &
			0,763 & 0,691 & 0,820 & 0,725 &
			2,688 \\
		LIN (3) & 22,5\% & 58,0\% & 7,6\% & 11,9\% &
			0,748 & 0,655 & 0,805 & 0,698 &
			2,526 \\
		LIN (4) & 22,5\% & 58,0\% & 7,6\% & 11,9\% &
			0,748 & 0,654 & 0,805 & 0,698 &
			2,528 \\
		LIN (5) & 22,6\% & 57,6\% & 7,9\% & 11,8\% &
			0,740 & 0,657 & 0,802 & 0,696 &
			2,478 \\
		LIN (6) & 23,5\% & 57,4\% & 8,2\% & 10,9\% &
			0,743 & 0,683 & 0,809 & 0,712 & 
			2,547 \\
		LIN (7) & 23,8\% & 56,9\% & 8,6\% & 10,7\% &
			0,734 & 0,690 & 0,807 & 0,711 & 
			2,516 \\
		LIN (8) & 23,1\% & 57,1\%&  8,5\% & 11,4\% &
			0,732 & 0,670 & 0,802 & 0,700 &
			2,482 \\
		POLY, $c=1$ (8) & 24,5\% & 57,7\% & 7,8\% & 9,9\% &
			0,758 & 0,711 & 0,822 & 0,734 &
			2,704 \\
		RBF (8) & 24,4\% & 58,2\% & 7,3\% & 10,0\% &
			0,769 & 0,708 & 0,826 & 0,737 &
			2,783 \\			
		POLY, $c=1$ (9) & 24,0\% & 57,6\% & 8,0\% & 10,4\% &
			0,751 & 0,697 & 0,816 & 0,723 &
			2,603 \\
		RBF (9) & 23,6\% & 58,2\% & 7,4\% & 10,9\% &
			0,761 & 0,684 & 0,817 & 0,721 &
			2,648 \\
		LIN (10) & 24,3\% & 57,2 \% & 8,3\% & 10,1\% &
			0,744 & 0,705 & 0,815 & 0,724 &
			2,582 \\
		RBF (11) & 23,5\% & 58,3\% & 7,2\% & 11,0\% &
			0,766 & 0,682 & 0,819 & 0,721 &
			2,703 \\
		POLY, $c=1$ (11) & 24,2\% & 58,0\% & 7,5\% & 10,3\% &
			0,763 & 0,702 & 0,822 & 0,731 &
			2,701 \\
		LIN (11) & 20,1\% & 56,7\% & 8,9\% & 14,3\% &
			0,694 & 0,584 & 0,768 & 0,634 &
			2,201 \\
		LIN (12) & 24,3\% & 57,2\% & 8,3\% & 10,1\% &
			0,744 & 0,705 & 0,815 & 0,724 &
			2,583 \\
		LIN (13) & 24,4\% & 57,2\% & 8,4\% & 10,0\% &
			0,744 & 0,709 & 0,816 & 0,726 &
			2,581 \\
		LIN (14) & 24,3\% & 57,2\% & 8,3\% & 10,1\% &
			0,744 & 0,705 & 0,815 & 0,724 &
			2,583 \\
		LIN (15) & 24,4\% & 57,1\% & 8,4\% & 10,0\% & 
			0,744 & 0,710 & 0,816 & 0,726 &
			2.578 \\
		
		
	\end{tabular}
	\caption{Rezultati metod, razvitih na podlagi metode podpornih vektorjev.}
	\label{tb:svm}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{r|l}
		\textbf{Oznaka} & \textbf{Opis} \\
		\hline
		(1) & Originalni nabor značilk. \\
		(2) & Dodane manjkajoče vrednosti. \\
		(3) & Filtrirane manjkajoče vrednosti in vse $e^x$. \\
		(4) & Filtrirane manjkajoče vrednosti in vse $x^2$. \\
		(5) & Filtrirane manjkajoče vrednosti, $x^2$ in vse $x^3$. \\
		(6) & Filtrirane manjkajoče vrednosti, $x^2$, $x^3$, $e^x$ in vse $\sqrt{x}$. \\
		(7) & Filtrirane manjkajoče vrednosti, $x^2$, $x^3$, $e^x$, $\sqrt{x}$ in vse $\log(x)$. \\
		(8) & Izbor najboljših značilk po eni spremenljivki: manjkajoče, $x^2$, $e^x$, $\sqrt{x}$. \\
		(9) & Nefitriran set vseh značilk po eni spremenljivki. \\
		(10) & Nefiltriran set $x_ix_j$ (435 značilk). \\
		(11) & Nabor značilk, kot jih predlaga en od zmagovalcev HiggsML Tim Salimans.\footnote{Tim Salimans, \url{https://github.com/TimSalimans/HiggsML/}} \\
		(12) & Nefiltriran set $x_i^2 + y_i^2$. \\
		(13) & Nefiltriran set $e^{x_i^2 + y_i^2}$. \\		
		(14) & Nefiltriran set $\sqrt{x_i^2 + y_i^2}$. \\
		(15) & Nefiltriran set $(1 + x_ix_j)^2$. \\
		(16) & Filtriran set značilk po eni in dveh spremenljivkah. \\
	\end{tabular}
	\caption{Nabor značilk pri metodi SVM.}	
\end{table}

\comment{Kombinacija s pospešenim gradientnim spustom}

% -----------------------------------------------------------------------------
% POGLAVJE: Zaključek
% -----------------------------------------------------------------------------
\chapter*{Zaključek}
\addcontentsline{toc}{chapter}{Zaključek}

% -----------------------------------------------------------------------------
% ZAKLJUČNE STRANI
% -----------------------------------------------------------------------------
\input{endpages}

% -----------------------------------------------------------------------------
% DODATEK
% -----------------------------------------------------------------------------
\input{appendix}


\end{document}
