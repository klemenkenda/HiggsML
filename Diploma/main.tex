% !TeX spellcheck = sl_SI
\documentclass[11pt,a4paper,openany]{book}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, fullpage}
\usepackage{lmodern}
\usepackage{epsfig}
\usepackage{setspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{parskip}
% \usepackage[dvips]{graphicx}
\usepackage[titletoc]{appendix}
\usepackage[dvips]{xy}
\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{soul}
\onehalfspacing

\begin{document}
\input{macros}

% -----------------------------------------------------------------------------
% UVODNESTRANI
% -----------------------------------------------------------------------------
\input{intropages}







% -----------------------------------------------------------------------------
% POVZETEK
% -----------------------------------------------------------------------------
\chapter*{Povzetek}
\addcontentsline{toc}{chapter}{Povzetek}

Plan dela:
\begin{itemize}
	\item \st{Uvod} + Opis HiggsML + Osnovno razumevanje fizikalnega dela + opis
	\item \st{Exploratory analysis + opis podatkov}
	\item Usposobitev baseline (predlaganih) metod
		\begin{itemize}
			\item Simple Window
			\item Naive Bayes + opis metode
			\item XGBoost + opis nagrajene metode
		\end{itemize}
	\item Osnove strojnega učenja + delo na ostalih metodah + opis metod
		\begin{itemize}
			\item \st{Logistic regression}
			\item \st{Perceptron}
			\item NN + opis zmagovalne metode
			\item SVM
		\end{itemize}
	\item Izboljšava metod
		\begin{itemize}			
			\item Na podlagi SVM
		\end{itemize}
	\item Zaključek + povzetek
\end{itemize} 

\vspace{1.3cm}
\noindent
{\large \bf Ključne besede:}

\vspace{0.5cm}
\noindent test, test, test


Abstract. Key words.







% -----------------------------------------------------------------------------
% UVOD
% -----------------------------------------------------------------------------
\chapter*{Uvod}
\addcontentsline{toc}{chapter}{Uvod}

Eksperimenta ATLAS in CMS sta leta 2012 objavila odkritje Higgsovega bozona\cite{Aad20121,Chatrchyan201230}. Odkritju je leta 2013 sledila Nobelova nagrada za fiziko, ki sta jo prejela François Englert in Peter Higgs. Obstoj delca, katerega vloga naj bi bila, da daje maso ostalim elementarnim delcem, je bil predviden pred skoraj 50 leti. Eksperimenti so potekali (in še vedno potekajo) na Velikem hadronskem trkalniku (Large Hadron Collider - LHC) v CERN-u (Evropski organizaciji za jedrske raziskave) v Ženevi\cite{ChallengeDoc}.

Higgsov bozon lahko razpade skozi različne procese, ki jim v fiziki osnovnih delcev pravimo kanali. Pri tem nastanejo novi delci. Higgsov bozon so najprej opazili v treh različnih razpadnih kanalih, v katerih vedno nastanejo pari bozonov. V naslednjem koraku je bilo potrebno najti dokaze o razpadu Higgsovega bozona v fermionske pare, predvsem v $\tau$ leptone in $b$ kvarke. Prvi dokazi o $H \rightarrow \tau^+\tau^-$ so bili predstavljeni v \cite{atlas2013}. 

\comment{Ali so rezultati vpliva $H \rightarrow \tau^+\tau^-$ že kje? Se to kaj navede? Koliko so izboljšali $\sigma$?}

Pri analizi eksperimentalnih podatkov je potrebno določiti relevantno območje faznega prostora izmerjenih značilk, v katerem je velika verjetnost, da smo naleteli na dogodke, ki nas zanimajo (v našem primeru na razpad $H \rightarrow \tau^+\tau^-$). V preteklosti so ta področja določali eksperti \textit{ročno}\cite{Adam-Bourdarios14}. Napredne klasifikacijske metode, ki temeljijo na strojnem učenju, pa se danes rutinirano uporabljajo za reševanje tega in podobnih problemov\cite{atlas2013}.

Področje dela tesno povezuje fiziko z računalništvom, natančneje - s strojnim učenjem. 

Namen diplomskega dela je predvsem seznaniti se z uporabo metod strojnega učenja pri ločevanju signala in ozadja pri razpadu $H \rightarrow \tau^+\tau^-$, preveriti in ovrednotiti različne klasifikacijske metode na podatkih simulatorja ATLAS in preveriti proces optimizacije teh metod, ki izhaja iz podatkov (in ne nujno domenskega znanja). Sekundarni cilj je razvoj lastne klasifikacijske metode, ki bo temeljila na metodi podpornih vektorjev (SVM).

Še enkrat velja izpostaviti, da se diplomsko delo nanaša predvsem na uporabo metod strojnega učenja pri analizi zahtevnih in analitično neobvladljivih problemov, in ne na fiziko osnovnih delcev in razlago standardnega modela (SM).

\comment{Če zmagovalna metoda na lepih podatkih doseže AMS 3,7, kako lahko na podlagi teh}
\comment{rezultatov pridemo do $\sigma = 5$??}

\section*{Izziv HiggsML}
\comment{Opis challenge-a.}
\comment{Globoko fizikalno razumevanje načeloma v tem izzivu ni pripomoglo k signifikantnemu izboljšanju}
\comment{klasifikacijskih modelov. Osnovne izpeljane vrednosti so zadoščale!}
\comment{Vir: https://www.kaggle.com/c/higgs-boson/forums/t/10350/how-physicists-fared}


\section*{Terminologija}
\addcontentsline{toc}{section}{Terminologija}

Ker je področje strojnega učenja in umetne inteligence v slovenskem prostoru majhno in ker se slovenska terminologija na tem področju še redkeje uporablja in je zato tudi strokovno podkovanemu bralcu manj znana, na tem mestu prilagamo kratek slovarček strokovnih terminov s področja strojnega učenja. Vsi prevodi temeljijo na \textit{Računalniškem slovarčku}, ki ga vzdržuje Odsek za inteligentne sistem na Inštitutu "Jožef Stefan"\footnote{\url{http://dis-slovarcek.ijs.si}}.

\input{slovar}

% -----------------------------------------------------------------------------
% POGLAVJE: Fizikalne osnove
% -----------------------------------------------------------------------------
\chapter{Fizikalne osnove}

\comment{Osnovni članek: \cite{AadScience2012}.}


\section{Razpad $H \rightarrow \tau^+\tau^-$}

\cite{Baldi2014} \cite{atlas2013}


\section{ATLAS detektor}
\cite{AadScience2012}


\section{Izpeljava relevantnih značilk}
Podpoglavje opisuje nekaj osnovnih enačb iz posebne teorije relativnosti in sledi predvsem izpeljavi novih značilk iz surovih podatkov. Izpeljane značilke si je moč ogledati v podpoglavju \ref{opis-podatkov}.

\subsection{Gibalna količina, masa in energija}
Fundamentalna enačba posebne teorije relativnostni
\begin{equation}
	E^2 = p^2c^2 + m^2c^4,
	\label{stirimoment}
\end{equation}
\comment{Strnad 3/39 (9)}
kjer je $E$ energija delca, $p$ njegova gibalna količina, $m$ lastna masa (masa v mirovanju) in $c$ hitrost svetlobe v praznem prostoru. V primeru, ko delec miruje (in ima s tem gibalno količino enako $0$), se enačba poenostavi v znano $E = mc^2$.

\comment{Relevantno za izpeljavo značilk! Navedba/izpeljava enačb za izpeljane vrednosti ... \cite{Adam-Bourdarios14}}
\comment{Mogoče smiselno citirati tudi Strnada 3?}

% -----------------------------------------------------------------------------
% POGLAVJE: Opredelitev problema
% -----------------------------------------------------------------------------
\chapter{Opredelitev problema}

Problem, s katerim sem se spopadel v diplomskem delu, sledi formulaciji na odprtem tekmovanju \textit{The Higgs Boson Machine Learning Challenge (HiggsML)}\footnote{ \url{http://www.kaggle.com/c/higgs-boson}}, ki temelji na rezultatih kolaboracije ATLAS\cite{Adam-Bourdarios14}. 


\section{Formalna opredelitev problema}

\comment{Notacijo spremeni, da bo konsistentna z Andrew Ng - spodaj.}

Naj ${\cal D} = \left\{({\mathbf x}_1, y_1, w_1), \dots, ({\mathbf x}_n, y_n, w_n) \right\}$ predstavlja učno podatkovno množico, kjer je $\mathbf{x}_i \in \mathbb{R}^d$ $d$-dimenzionalni vektor značilk, $y_i \in \{\text{b, s}\}$ je oznaka, $w_i \in \mathbb{R}^+$ pa je nenegativna utež. Naj bosta ${\cal S} = \{i : y_i = \text{s}\}$ in ${\cal B} = \{i : y_i = \text{b}$ množici indeksov dogodkov, ki predstavljajo signal in ozadje, $n_\text{s} = |{\cal S}|$ in $n_\text{b} = |{\cal B}|$ pa naj označujeta števili simuliranih dogodkov, ki predstavljata signal in ozadje.

Podatki, na katerih se učimo, so simulirani (glej poglavje \ref{analiza-podatkov}) in se razlikujejo od izmerjenih. Razmerje $n_\text{s} / n_\text{b}$ v podatkih tako ne odraža dejanskega razmerja dogodkov $P(y = s) / P(y = b)$. Glede ne nizko verjetnost, da pri nekem naravnem dogodku gre za signal \cite{Adam-Bourdarios14}, je tako učna podatkovna množica precej bolj uravnotežena in omogoča metodam, da se lahko naučijo razlikovati med dogodki, ki predstavljajo ozadje in tistimi, ki predstavljajo signal.

Podatek, ki ga da simulacija, je tudi utež $w_i$, ki je mera za pomembnost nekega simuliranega dogodka. Ker je optimizacijska funkcija (\ref{en:ams}) odvisna od \textit{nenormalizirane vsote} uteži in ker želimo, da je naš sistem invarianten na števili simuliranih dogodkov $n_s$ in $n_b$, moramo vsoto vsake podatkovne množice (učne, validacijske ...) za vsak razred (signal ali ozadje) fiksirano:
\begin{equation}
\sum_{i \in \cal{S}}{w_i} = N_s
\qquad\text{in}\qquad
\sum_{i \in \cal{B}}{w_i} = N_b
\end{equation}
Normalizacijski konstanti $N_s$ in $N_b$ imata fizikalni pomen in predstavljata \textit{pričakovano število} dogodkov ki predstavljajo signal in ozadje, med intervalom zajema podatkov (v našem primeru leta 2012). Individualne uteži so proporcionalne pogojnim gostotam, deljenimi z instrumentalnimi gostotami, uporabljenimi na simulatorju.

\begin{equation}
	w_i = \left\{\begin{array}{r}
		p_s(\textbf{x}_i)/q_s(x_i),\qquad \text{if}\;y_i = s \\
		p_b(\textbf{x}_i)/q_b(x_i),\qquad \text{if}\;y_i = b 
	\end{array}
	\right.,
\end{equation}
kjer sta
\begin{equation*}
p_s(\textbf{x}_i) = p(\textbf{x}_i|y = s) \qquad in \qquad p_b(\textbf{x}_i) = p(\textbf{x}_i|y = b)
\end{equation*}
pogojni gostoti signala in ozadja ter $q_s(\textbf{x}_i)$ in $q_b(\textbf{x}_i)$ instrumentalni gostoti.

Naj bo $g : \mathbb{R}^d \rightarrow \{b, s\}$ poljubna klasifikacijska funkcija. Naj bo izbrano področje ${\cal G} = \{\textbf{x} : g(\textbf{x}) = s\}$ množca točk, klasificirana kod signal, in naj $\hat{\cal G}$ označuje množico indeksov točk, ki jih $g$ izbere (klasificira kot signal).
\begin{equation*}
\hat{\cal G} = \{ i : \textbf{x}_i \in {\cal G}\} = \{ i : g(\textbf{x}_i) = s \}.
\end{equation*}


\section{AMS metrika}
\label{sc:ams}
\begin{equation}
\text{AMS} = \sqrt{2 \left( ( s + b + b_{reg} ) \ln \left( 1 +  \frac{s}{b + b_{reg}} \right) - s \right) }
\label{en:ams}
\end{equation}

% -----------------------------------------------------------------------------
% POGLAVJE: Metode strojnega učenja
% -----------------------------------------------------------------------------
\chapter{Strojno učenje}

\cite{Mitchell1997}, \cite{Witten2005}.

\section{Raziskovalna analiza podatkov}
Raziskovalna analiza podatkov predstavlja enega od pristopov k začetni analizi množice podatkov predvsem s pomočjo vizualnih metod. Vizualne metode temeljijo na sposobnosti človeških možganov, da prepoznajo strukturo v podatkih. Namen analize je pregledati osnovne lastnosti podatkov, ki pomagajo pri določanju scenarijev za implementacijo metod strojnega učenja. Poleg omenjenega so vizualne metode na področju rudarjenja podatkov pomembne tudi zato, ker je mogoče z njihovo pomočjo odkriti nove, nepričakovane zakonitosti v podatkih. Metode raziskovalne analize imajo pri visoko-dimenzionalnih naborih podatkov očitne omejitve. Opisi v diplomskem delu so povzeti po \cite{hand2001}.

Metoda odraža pristop k reševanju problema, ki temelji na podatkih in ne na predhodnem znanju. S pomočjo takega pristopa lahko odkrijemo nove značilnosti v podatkih in s tem novo znanje. Metodologija je bila razvita že v začetku šestdesetih let prejšnjega stoletja, je pa svoj preporod doživela z razvojem računalništva, ki je močno olajšalo izdelavo vizualizacij. 

\textbf{Osnovni pregled značilk} se nanaša na pregled po eni posamezni značilki. Zajema \textbf{pregled manjkajočih vrednosti} v podatkih, \textbf{pregled osnovnih agregatov} in natančnejši \textbf{pregled porazdelitev} določenih značilk s pomočjo histogramov. Pregled osnovnih agregatov zajema: povprečja, standardno deviacijo, minimum, maksimum, število vrednosti, kvantile ... Ponazoritev posamezhnih značilk s pomočjo histogramov pa nam da lepši grafični pregled in informacijo o porazdelitvi posamezne značilke.

\textbf{Razpršeni grafi} se nanašajo na 2 značilki. Podatke ponazorimo s točkami, katerih eno koordinato predstavlja vrednost ene, drugo pa druge značilke. Grafe ponavadi prikažemo v matriki 2, 3 ali 4 različnih značilk, tako da jih lahko primerjamo med sabo. Takšen prikaz nam lahko razkrije skrito strukturo v podatkih, ki je po pregledu ene same značilke ne zasledimo. V primeru klasifikacije lahko npr. opazimo, če določena kombinacija značilk bolje loči oba razreda kot ena sama, prav tako pa lahko s temi grafi opazimo medsebojno korelacijo posameznih značilk, ki se odraža na urejenosti/razpršenosti točk.

\textbf{Korelacijska matrika značilk} je zelo učinkovita metoda za hiter in celosten pregled nad podatki. V matriki dimenzije $d \times d$, kjer je $d$ število značilk, s pomočjo barvne lestvice ponazorimo korelacijo med parom značilk. S pomočjo kompletnega pivotiranja, ki ga naredimo s pomočjo hierarhičnega grozdenja značilk, lahko značilke, ki so si med seboj podobne glede na korelacijo z drugimi, smiselno uredimo. Metoda je še zlasti uporabna pri izločanju linearno odvisnih značilk, ki modelom ne prinašajo dodatne vrednosti. 

\textbf{Analiza glavnih komponent} (\textit{Principal component analysis}, PCA) je definirana kot ortogonalna linearna transformacija, ki transformira nabor podatkov v nov koordinatni sistem, in sicer tako, da komponenta z največjo varianco neke projekcije podatkov leži na prvi koordinati, druga največja na drugem mestu in tako naprej. Metoda je poenostavitev metode večdimenzionalnega skaliranja (\textit{Multi-dimensional scaling}, MDS).

Praktična uporaba opisanih metod z razultati na podatkih iz detektorja ATLAS je opisana v poglavju \ref{analiza-podatkov}.

\section{Metode za nadzorovano učenje}

Na področju strojnega učenja v grobem ločimo dva različna razreda metod: (1) \textbf{metode za nadzorovano učenje} in (2) \textbf{metode za nenadzorovano učenje}. Zadnji razred metod se ponavadi izvaja na podatkih, ki ne vsebujejo oznake $y^{(i)}$, torej parametra, ki nas zanima (npr. oznaka razreda). Zakonitosti iščemo torej samo v strukturi, ki jo oblikuje množica točk $x^{(i)}$ v faznem prostoru. Osnovni primer take metode je grozdenje, ki poskuša točke povezati s smiselne zaključene množice - grozde. Metoda grozdenja je lahko tudi učinkovita metoda generiranja novih značilk, pri čemer oznaka grozda služi kot dodatna značilka v nadzorovanem učnem problemu.

Metode za nadzorovano učenje (1) delujejo na podatkih z znanim parametrom, ki nas zanima ($y^{(i)}$). Ločimo regresijske in klasifikacijske metode. Pri \textbf{regresijskih metodah} je ponavadi parameter $y^{(i)} \in \mathbb{R}$. Cilj regresijskih metod je za neznano točko $x^{(i)}$ oceniti realno vrednost $y^{(i)}$. 

V tipičnem \textbf{klasifikacijskem problemu}, ki je tudi sicer predmet diplomskega dela, imamo množico vektorjev v prostoru $x^{(i)} \in \mathbb{R}^d$ (vektor značilk) in pripadajočih oznak $y^{(i)} \in {1, 2, 3, \ldots, N}$, kjer $N$ označuje število različnih razredov. Oznaka $y^{(i)}$ nam pove, v kateri razred spada vektor $x^{(i)}$. Cilj klasifikacijskega problema je predvideti oznake neznanih vektorjev, in sicer tako, da bo pri tem napaka najmanjša.

Za binarne klasifikacijske probleme velja $N = 2$. V takih primerih obstajata 2 razreda: pozitivni in negativni. S takšnim problemom se srečujemo tudi v tem diplomskem delu.


\section{Pregled klasifikacijskih metod}

Pri pregledu klasifikacijskih metod sem se omejil na naslednje metode: osnovne metode, ki prinašajo dodaten vpogled v podatke (logistična regresija), osnovne metode, ki jih je predlagal organizator (preprosto okno, naivni Bayesov klasifikator, pospešena odločitvena drevesa), zmagovalno metodo (globoke nevronske mreže) in na metodo, katere uporabo za dani problem sem želel v diplomskem delu sam raziskati - metodo podpornih vektorjev. 


\subsection{Logistična regresija}

Linearno regresijo je moč, ko imamo opravka z numeričnimi značilkami, uporabiti kot klasifikacijsko metodo. Pravzaprav lahko vsako regresijsko tehniko uporabimo za klasifikacijo. Prenos regresijske tehnike v klasifikacijsko domeno lahko naredimo s pomočjo preprostega trika, in sicer da pozitivno klasificiranim točkam pripišemo vrednost 1, negativno klasificiranim pa vrednost 0. Ko poskusimo s pomočjo naučenega modela klasificirati neznano točko, lahko vrednost, ki jo vrne linearna regresija, interpretiramo kot funkcijo članstva v določenem razredu. \cite{Witten2005}

Težava tega pristopa je, da se funkcija članstva ne nanaša na verjetnost pripadnosti vzorca določenemu razredu (saj je lahko večja od 1 ali manjša od 0), hkrati pa je binarna porazdelitev vrednosti točk daleč od normalne, ki jo zahteva metoda.

\comment{Povzeto po Andrew Ng, Stanford University on-line course, 10. 10. - 12. 12. 2011. Kako citirati?}
\comment{\url{http://cs229.stanford.edu/notes/cs229-notes1.pdf}}

\textit{Logistična regresija} je statistična metoda, ki nima težav iz zgornjega odstavka. Namesto, da bi poskusila vrednosti 0 in 1 aproksimirati neposredno, uporabi za ta namen transformacijsko funkcijo ($[0, 1] \rightarrow \mathbb{R}$). Katerokoli realno število pretvori na interval med 0 in 1.

Standardno logistično funkcijo zapišemo kot
\begin{equation}
	\label{en:standardna_log_funkcija}
	h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}},
\end{equation}
kjer se
\begin{equation}
	g(z) = \frac{1}{1 + e^{-z}}
\end{equation}
imenuje \textbf{logistična funkcija} ali {sigmoid}. Graf funkcije je prikazan na sliki \ref{sl:standardna_logisticna}.
$h(x)$ predstavlja linearno kombinacijo značilk $h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n$. Če privzamemo $x_0^{(i)} = 1$, lahko zapišemo $h(x) = \theta \cdot x$.

Interpretacija hipoteze je verjetnostna. Vrednost hipoteze je $P(y=1|x;\theta)$ - torej verjetnost, da je $y = 1$ pri danem $x$ in $\theta$.

Naloga učnega algoritma je določitev parametrov $\theta_i$. Najprej moramo definirati logistično minimizacijsko funkcijo $J(\theta)$. Ta funkcija je odvisna od razlike med hipotezo in dejansko vrednostjo točk. Pri linearni regresiji funkcijo zapišemo kot
\begin{equation}
	J(\theta) = \frac{1}{m} \sum^m_{i = 1} \frac{1}{2} \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2.
\end{equation}
Parametre $\theta$ izberemo tako, da je vsota kvadratnih členov kar najmanjša. Pri logistični regresiji sumand nadomestimo s \textit{cost} funkcijo $\text{Cost}(h_\theta(x), y)$. Zapišimo \textit{cost} funkcijo
\begin{equation}
	\text{Cost}(h_\theta(x), y) = \left\{
	\begin{array}{rl}
		-\log(h_\theta(x)) & \text{če je} \; y = 1 \\
		-\log(1 - h_\theta(x)) & \text{če je} \; y = 0
	\end{array}
	\right.
\end{equation}
Vemo, da je $y \in \{0, 1\}$. Zdaj lahko v eni sapi zapišemo $J(\theta)$
\begin{equation}
	J(\theta) = -\frac{1}{m} \left[ 
		\sum^m_{i = 1}
			y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
	\right].
	\label{en:j_theta}
\end{equation}
Naša učna naloga je poiskati tak $\theta$, da bo $J(\theta)$ najmanjši. Napoved za nov nepoznani $x$ izračunamo s pomočjo enačbe (\ref{en:standardna_log_funkcija}), ki nam predstavlja verjetnost, da dani primerek spada v pozitivni razred $P(y = 1 | x;\theta)$. $\theta$ bomo poiskali s pomočjo minimizacije $J(\theta)$, in sicer s pomočjo gradientnega spusta.

Ena pomembnih tehnik pri zagotavljanju uspešnega gradientnega spusta je skaliranje značilk. S tem se izognemo nepotrebnim oscilacijam, ki upočasnjujejo konvergenco. Oscilacije so posledica dejstva, da je velikostni red nekaterih značilk precej nižji od ostalih.

Osnovni algoritem gradientnega spusta je zelo preprost. Potrebno je izračunati parcialne odvode $\frac{\partial}{\partial x_i}J(\theta)$. Upoštevajoč enačbo (\ref{en:j_theta}) lahko zapišemo posamezni korak naše minimizacije
\begin{equation}
	\theta_j := \theta_j - \alpha \sum^m_{i = 1}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)},
	\label{en:log_reg_update}
\end{equation}  
pri čemer velja $j \in {1, 2, ...,  d}$, popravki pa se izvršijo simultano (torej ne vplivajo drug na drugega).

\subsection{Naivni Bayesov klasifikator}

\comment{http://cs229.stanford.edu/notes/cs229-notes2.}

Pri logistični regresiji (podobno tudi pri perceptronu, glej spodaj) smo poskusili predvideti $p(y|x;\theta)$, pogojno porazdelitev $y$ pri danem $x$. Pri logistični regresiji smo verjetnost aproksimirali kar z logistično funkcijo $h_\theta(x) = g(\theta^Tx)$, kjer je $g$ sigmoidna funkcija. Pri klasifikacijskem problemu se tak algoritem nauči določiti mejo med različnimi razredi točk. Takim algoritmom pravimo \textbf{diskriminativni}.

Naivni Bayesov klasifikator je \textbf{generativni} algoritem. Ta se poskuša naučiti modelirati $p(x|y)$ (in $p(y)$). Povedno preprosteje, algoritem poskusi modelirati porazdelitev posamezne značilke v primeru, ko meritve spadajo v točno določen razred. Lepa ilustracija tega pristopa so histogrami, ki smo jih pridelali v raziskovalni analizi podatkov, je npr. na sliki \ref{sl:histogrami}.

Ko znamo izračunati $p(y)$ (imenovan tudi \textbf{class prior}) in $p(x|y)$, lahko uporabimo Bayesovo pravilo, da izpeljemo porazdelitev $p(y|x)$.
\begin{equation}
	p(y|x) = \frac{p(x|y)p(y)}{p(x)}.
\end{equation}
Imenovalec lahko izračunamo kot $p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$, v resnici pa ga ni potrebno nikoli izračunati, saj dejansko v klasifikacijskem algoritmu iščemo
\begin{equation}
	\arg \max_y p(y|x) = \arg \max_y \frac{p(x|y)p(y)}{p(x)} = \arg \max_y p(x|y)p(y).
\end{equation}
Privzemimo, da je porazdelitev $p(x|y)$ normalna. V $d$ dimenzijah takšno porazdelitev parametriziramo s pomočjo povprečja $\mu \in \mathbb{R}^d$ in kovariančne matrike $\Sigma \in \mathbb{R}^{d \times d}$, kjer je $\Sigma \geq 0$ simetrična in pozitivno semi definitna Porazdelitev zapišemo kot
\begin{equation}
	p(x;\mu,\Sigma) = {\cal N}(\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp \left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right),
\end{equation}
kjer $|\Sigma|$ označuje determinantno matrike $\Sigma$.

Model zapišimo tako, da ima $p(y)$ Bernoullijevo porazdelitev, $p(x|y)$ pa normalno:
\begin{eqnarray}
	p(y) &=& \phi^y(1-\phi)^{1-y} \\
	p(x|y = 0) &=& {\cal N}(\mu_0;\Sigma) \\
	p(x|y = 1) &=& {\cal N}(\mu_1;\Sigma)
\end{eqnarray}

\comment{Manjka!!!}

Naivni Bayesov klasifikator se uporablja predvsem v primerih, ko imamo opravka z binarnimi značilkami (npr. pri obravnavni teksta, najbolj značilna uporaba metode je pri klasifikaciji nezaželene elektronske pošte), uporaba v primerih, ko je značilka $x_i \in \{1, 2, ..., k_i\}$ preprosta, če uporabimo ustrezno porazdelitev za $p(x_i|y)$. Pred uporabo algoritma je zato potrebno zvezne značilke ustrezno diskretizirati.

Pri naivnem Bayesovem klasifikatorju privzamemo, da so značilke $x_i$ med seboj pogojno neodvisne. Pogojna neodvisnost pomeni, da bomo v primeru, ko bomo imeli opravka z npr. pozitivno klasificirano točko ($y = 1$), ki bo imela neki vrednosti značilk $x_1 = a_1$ in $x_2 = a_2$ privzeli, da naše vedenje o $x_1$ v ostalih primerih ne bo vplivalo na vedenje o $x_2$. To pomeni, da bo veljalo $p(x_1|y) = p(x_1|y, x_2)$. 

Sedaj lahko izračunamo verjetnost
\begin{equation}
	\begin{split}
	p(x_1,...,x_d|y) &= p(x_1|y)p(x_2|y, x_1)p(x_3|y,x_1,x_2) \cdots p(x_d|y,x_1,...x_{d-1}) \\
	& = p(x_1|y)p(x_2|y)p(x_3|y) \cdots p(x_d|y) \\
	& = \prod_{i=0}^{d} p(x_i|y).
	\end{split}
\end{equation}
Predpostavko pogojne neodvisnosti je sicer težko izpolniti, vendar končni algoritem dobro deluje na veliko različnih problemih.

Dobra lastnost algoritma je, da za relativno dobro delovanje potrebuje zelo malo primerov v učni podatkovni množici, kar pa v primeru obsežne zbirke podatkov, s katero imamo opravka v diplomskem delu, ni odločilna prednost.

\subsection{Metoda podpornih vektorjev}

Denimo, da imamo binarni klasifikacijski problem in da sta razreda točk v našem prostoru linearno separabilna. V večini takih primerov obstaja neskončno mnogo različnih hiperravnin, ki ločujejo med sabo oba razreda (glej sliko \ref{svmseparable}).

\begin{figure}[ht]
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_1.pdf}
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_2.pdf}
	\includegraphics[width=5.2cm]{methods/svm/svm_axes_3.pdf}	
	
	\caption{Veliko različnih rešitev za hiperravnino, ki ločuje razreda primerkov.}
	\label{svmseparable}
\end{figure}

Metoda podpornih vektorjev (\textit{angl. Support Vector Machine} oz. SVM) se imenuje tudi klasifikator maksimalnega razmika. Prej opisani problem binarne klasifikacije s tem dobi enolično rešitev. Primer klasifikatorja z maksimalnim razmikom je ilustriran na sliki \ref{svmmaxclass}.

\comment{Pripravi sliko!}

\comment{Vključi komentar o več dimenzijah in uporabo kernelov.}

Hiperravnino, ki ločuje različna razreda primerkov označimo s $H$. Enačbo ravnine lahko zapišemo kot
\begin{equation}
  w^T x + b = 0,
\end{equation}
kjer je $w$ normalni vektor (v našem kontekstu ga bomo imenovali \textit{vektor uteži}), $b$ pa odmik ravnine v izhodišču.

Za enolično reprezentacijo hiperravnine je potrebno normalizirati faktorja $w$ in $b$. Taki reprezentaciji pravimo tudi kanonična reprezentancija.

\comment{Pride nekje do AMS 2,7.}
\comment{https://www.kaggle.com/c/higgs-boson/forums/t/10165/has-anyone-tried-the-support-vector-machine-for-this-problem}
\comment{Tu se splača igrat s kerneli.}

\subsection{Nevronske mreže}
\comment{Deep NN - zmagovalna metoda. Ansambli.}

\subsubsection{Perceptron}
Razpravo o nevronskih mrežah začenjamo pri algoritmu, imenovanem \textbf{perceptron}. V 60-ih letih prejšnjega stoletja so ta algoritem uvedli kot osnovni model delovanja nevronov v možganih. Kljub temu, da se razprava o perceptronu lepo navezuje na razpravo o logistični regresiji, je narava tega algoritma precej drugačna od ostalih. Predvsem je na podlagi rezultatov tega algoritma težko najti interpretacije, ki bi temeljile na verjetnosti ali oceni maksimalne podobnosti (maximum likelihood).

V primeru, da logistično regresijo spremenimo tako, da funkcijo $g(z)$ zapišemo v obliki Heavisidove funkcije:
\begin{equation}
	g(z) = \left\{
		\begin{array}{ll}
			1 & \text{če je} \; z \geq 0 \\
			0 & \text{če je} \; z \le 0
		\end{array}
	\right.
\end{equation}
Podobno kot v primeru logistične regresije določimo $h_\theta(x) = g(\theta^Tx)$. Pravilo za gradientni spust ostane enako definiciji v (\ref{en:log_reg_update}).


\subsubsection{Umetne nevronske mreže}

\subsubsection{Globoke nevronske mreže}

\subsubsection{Ansambli}

\subsection{Pospešena odločitvena drevesa}
\comment{TMVA boosted trees? XGBoost - metoda, ki je manj potratna.}
\comment{http://higgsml.lal.in2p3.fr/software/hep-tmva-kit/}
\comment{http://higgsml.lal.in2p3.fr/software/multiboost/}
\comment{https://github.com/dmlc/xgboost}


\section{Evalvacija klasifikacijskih metod}
Poleg metrike, ki je razložena v razdelku \ref{sc:ams} in je specifična problemu ločevanja signala in ozadja v primeru podatkov iz detektorja ATLAS, je v splošnem za razumevanje delovanja in izbiro klasifikacijskih metod priporočljivo uporabiti še nekaj metrik. Pregled metrik je povzet po \cite{wiki:precision_and_recall}.

\begin{itemize}
	\item \textbf{pravilno pozitivni (\textit{true positive}, TP)}, predstavlja število pravilno klasificiranih pozitivnih dogodkov (signal)
	\item \textbf{pravilno negativni (\textit{true negative}, TN)}, predstavlja število pravilno klasificiranih negativnih dogodkov (ozadje)
	\item \textbf{napačno pozitivni (\textit{false positive}, FP)}, predstavlja število napačno klasificiranih negativnih dogodkov (ozadje, klasificirano kot signal)
	\item \textbf{napačno negativni (\textit{false negative}, FN)}, predstavja število napačno klasificiranih pozitivnih dogodkov (signal, klasificiran kot ozadje)
	\item \textbf{natančnost (\textit{precision} ali \textit{positive prediction value}, PPV)} (pozitivna napovedna vrednost), definirana kot 
	\begin{equation}	
	PPV = \frac{TP}{TP + FP}
	\end{equation}
	\item \textbf{priklic (\textit{recall} ali \textit{true positive rate}, TNR)} (ali občutljivost), definiran kot
	\begin{equation}	
	TNR = \frac{TP}{TP + FN}
	\end{equation}
	\item \textbf{$F_1$ score} povezuje natančnost in priklic v geometrijskem povprečju. 
	\begin{equation}
	F_1 = 2 \cdot \frac{PPV \cdot TNR}{PPV + TTR} = \frac{2TP}{2TP + FP + FN}
	\end{equation}
	\item \textbf{točnost (\textit{accuracy}, ACC)}
	\begin{equation}
	ACC = \frac{TP + TN}{P + N}
	\end{equation}
\end{itemize}

\comment{Teorija je precej bolj podrobna, vendar se ne bi poglabljal.}

\comment{Mogoče bi moral razložiti tudi ROC krivuljo.}

% -----------------------------------------------------------------------------
% POGLAVJE: Podatki
% -----------------------------------------------------------------------------
\chapter{Podatki}
\label{analiza-podatkov}
Podatki, ki sem jih uporabil v diplomskem delu, so bili uporabljeni za izvedbo \textit{HiggsML Challenge}. Podatki predstavljajo dogodke, ki so bili simulirani na uradnem simulatorju polnega detektorja kolaboracija ATLAS\cite{Adam-Bourdarios14}. Podatki iz simulatorja imajo lastnosti, ki posnemajo statistične lastnosti dejanskih dogodkov - tako signala, kakor tudi ozadja.

Vzorec signala zajema dogodke, v katerih naj bi nastal Higgsov bozon (katrega masa je bila nastavljena na $m_H = 125\,\text{GeV}$). Vzorec, ki predstavlja ozadje, vsebuje dogodke, ki odražajo druge znane procese, ki lahko proizvajajo dogodke z vsaj enim elektronom ali mionom in hadronskim tau, ki posnemajo signal.

V simuliranih podatkov so predstavljeni samo trije procesi ozadja. Prvi prihaja od razpada $Z \rightarrow \tau^+\tau^-$ (masa z bozona je $m_Z = 91.2\,\text{GeV}$). Ta razpad proizvaja dogodke s topologijo, ki je zelo podobna tisti pri razpadu Higgsovega bozona. Druga množica dogodkov vsebuje par $t$ kvarkov, ki imata lahko med svojimi razpadnimi produkti lepton ali hadronski tau. Tretja množica vsebuje razpad $W$ bozona, kjer lahko nastaneta en elektron ali mion in hadronski tau. 

\section{Opis podatkov}

V učni podatkovni množici je na voljo 250.000 dogodkov, od teh jih je 85.667 predstavljalo signal, 164.333 pa ozadje. Vsak dogodek je predstavljen z $d = 30$ značilkami, ki so dodatno opisane v \ref{sec:opis-znacilk}, in 3 metapodatki: oznako razreda ($s$ - signal ali $b$ - ozadje), utežjo $w_i$ in zaporedno številko dogodka. Raziskovalna analiza podatkov je predstavljena v \ref{sec:raziskovalna-analiza}.

Potrjevalna podatkovna množica vsebuje 100.000 dogodkov, ki pa niso opremljeni z oznako razreda ali utežjo $w_i$. 

\subsection{Opis značilk}
\label{sec:opis-znacilk}
Podatki zajemajo $d = 30$ značilk, katerih opis je povzet po \cite{Adam-Bourdarios14}. Stolpci s predpono PRI (ki označuje \textit{PRImitives}) predstavlja \textit{surove} podatke o trkih, kot jih izmeri detektor. Stolpci s predpono DER (ki označuje \textit{DERived}) predstavljajo vrednosti, izpeljane iz primitivnih lastnosti. Količine so določili sodelavci kolaboracije ATLAS z namenom, da bi pomagale definirati relevantna področja faznega prostora za klasifikacijske metode.

Stolpci brez predpone imajo posebno vlogo in se ne uporabljajo kot značilke:

\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item [EventId] 	Zaporedna številka dogodka.
	\item [Weight]  	Utež dogodka $w_i$, kakor je opisana v podpoglavju \ref{utez}.
	\item [Label] 		Oznaka dogodka (niz) $y_i \in \{\text{s}, \text{b}\}$ (s za signal, b za ozadje).	
\end{description}
\end{changemargin}

Stolpci s predpono \textbf{PRI} označujejo \textit{surove} podatke, ki jih detektor izmeri pri trku:
\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item[PRI\_tau\_pt] Prečni (transverse???) moment $\sqrt{p_x^2 + p_y^2}$ hadronskega tau.
	\item[PRI\_tau\_eta] Psevdorapidnost $\eta$ hadronskega tau.
	\item[PRI\_tau\_phi] Azimutalni kot $\phi$ hadronskega tau.
	
	\item[PRI\_lep\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ leptona (elektrona ali miona).
	\item[PRI\_lep\_eta] Psevdorapidnost $\eta$ leptona.
	\item[PRI\_lep\_phi] Azimutalni kot $\phi$ leptona.
	
	\item[PRI\_met] Manjkajoča prečna energija $\vec{E}_T^{\text{miss}}$.
	\item[PRI\_met\_phi] Azimutalni kot $\phi$ manjkajoče prečne energije.
	
	\item[PRI\_met\_sumet] Celotna prečna energija v detektorju.
	
	\item[PRI\_jet\_num] Število vseh žarkov ($\in {1, 2, 3}$), vednosti, večje od $3$ so bile zamenjane s 3
	
	\item[PRI\_jet\_leading\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ glavnega žarka, t. j. žarka z največjo prečno energijo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	\item[PRI\_jet\_leading\_eta] Psevdorapidnost $\eta$ glavnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	\item[PRI\_jet\_leading\_phi] Azimutalni kot $\phi$ glavnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} = 0$).
	
	\item[PRI\_jet\_subleading\_pt] Prečni moment $\sqrt{p_x^2 + p_y^2}$ prvega pomožnega žarka, t. j. žarka z drugo največjo prečno energijo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	\item[PRI\_jet\_subleading\_eta] Psevdorapidnost $\eta$ prvega pomožnega žarka, t. j. žarka z drugo največjo (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	\item[PRI\_jet\_subleading\_phi] Azimutalni kot $\phi$ prvega pomožnega žarka (vrednost ni definirana pri vrednosti značilke $\textbf{PRI\_jet\_num} \le 0$).
	
	\item[PRI\_jet\_all\_pt] Skalarna vsota prečnih momentov vseh žarkov pri dogodkih.
\end{description}
\end{changemargin}

Stolpci s predpono \textbf{DER} označujejo iz surovih podatkov izpeljane količine:
\begin{changemargin}{0.5cm}{0.0cm} 
\begin{description}
	\item[DER\_mass\_MMC] Ocenjena masa kandidata za Higgsov bozon $m_H$, pridobljena s pomočjo (\comment{MC?}) verjetnostne integracije po faznem prostoru (vrednost ni definirana v primeru, da topologija dogodka preveč odstopa od pričakovane)
	\item[DER\_mass\_transverse\_met\_lep] Prečna masa med manjkajočo prečno energijo in leptonom.
	\item[DER\_mass\_vis] Invarianta masa hadronskega $\tau$ in leptona.
	\item[DER\_pt\_h] Modulus ... \comment{uf, preveri!}
	\item[DER\_deltaeta\_jet\_jet] Absolutna vrednost psevdorapidnostne separacije med obema žarkoma (vrednost ni definirana v primeru $PRI\_jet\_num \le 1$).
	\item[DER\_mass\_jet\_jet] Invariantna masa obeh žarkov (vrednost ni definirana pri $PRI\_jet\_num \le 1$).
	\item[DER\_prodeta\_jet\_jest] Produkt psevdorapidnosti obeh žarkov (vrednost ni definirana pri $PRI\_jet\_num \le 1$).
	
\end{description}
\end{changemargin}

\comment{Vse izpeljane vrednosti so definirane v dodatku! Referenciranje na ustrezne enačbe!}

\section{Rezultati raziskovalne analiza podatkov}
\label{sec:raziskovalna-analiza}

\subsection{Manjkajoči podatki}

, tabela \ref{manjkajoče} pa predstavlja dogodke z manjkajočimi vrednostmi določenih značilk.

\begin{table}[ht]
	\centering
	\begin{tabular}{ccc}
		\hline
		{} & \textbf{signal} & \textbf{ozadje} \\
		\hline
		brez curkov & $81002\;(29,6\%)$ & $239967\;(45,3\%)$ \\
		1 curek & $88189\;(32,3\%)$ & $159071\;(30,2\%)$ \\
		vsi curki & $169191\;(61,9\%)$ & $398139\;(75,5\%)$ \\
		\hline
		Higgsova masa & $9011\;(3,3\%)$ & $114925\;(21,8\%)$
		
	\end{tabular}
	\caption{Manjkajoče značilke, število dogodkov in odstotek v razred.}
	\label{manjkajoče}
\end{table}


\comment{Večina gre v dodatek.}

	
% -----------------------------------------------------------------------------
% POGLAVJE: Rezultati osnovnih metod
% -----------------------------------------------------------------------------
\chapter{Rezultati osnovnih metod}

Vse implementacije sem naredil v programskem jeziku Python z uporabo paketa \texttt{scikit-learn}\cite{scikit-learn}. Za pospešena odločitvena drevesa sem uporabil nagrajeni paket XGBoost\cite{chen2014}.

	
\section{Preprosto okno}
\comment{http://higgsml.lal.in2p3.fr/software/simplest-python-kit/}

\section{Naivni Bayesov klasifikator}

\section{Multiboost}

	
% -----------------------------------------------------------------------------
% POGLAVJE: Izboljševanje metod in razvoj lastne metode
% -----------------------------------------------------------------------------
\chapter{Razvoj lastne metode}
\label{razvoj lastne metode}

Metodi, ki sta se pri reševanju danega problema odrezali najbolje, sta bili metodo globokih nevronskih mrež in pa pospešena odločitvena drevesa. Kljub temu, da je bil najboljši rezultat $AMS$ dosežen s pomočjo globokih nevronskih mrež, je izbrana metoda pri ločevanju signala od ozadja na projektu ATLAS metoda, ki temelji na pospošenih odločitvenih drevesih. Značilnost obeh metod je, da v sistem vpeljeta nelinearnost. 

Nekaj poročil je bilo o uporabi metode podpornih vektorjev (SVM)\footnote{\url{https://www.kaggle.com/c/higgs-boson/forums/t/10165/}}, vendar so tekmovalci poročali o precej nižjem rezultati, kakor je bil npr. osnovni rezultat metode pospešenih odločitvenih dreves.

Nelinearnost lahko v SVM modele pripeljemo s pomočjo jeder (\textit{kernel}) ali neposredno - s pomočjo generiranjem večjega nabora novih značilk, ki temeljijo na izvirnih značilkah. V grobem lahko zapišemo $x_j = f(x_{k_1}, x_{k_2}, ..., x_{k_l})$, pri čemerj je $l$ celo število in indeks izpeljane značilke $j > d$. Ponavadi velja $l \in \{1, 2, 3\}$. Funkcija $f$ je lahko poljubna. Pri vrednosti $l = 1$ lahko vpeljemo kvadrat, kub, logaritem, eksponent ali kakšno drugo podobno funkcijo. Pri večjem naboru se osredotočimo predvsem na produkte, vsote, diference, eksponente značilk v kombinaciji s prej omenjenimi funkcijami. 

Pri generiranju novih značilk šteje iznajdljivost in inovativnost. Zanimiv pristop je npr. uporaba metode grozdenja (npr. KMeans) in uporaba zaporedne številke grozda kot nove značilke.

Lepa lastnost metode SVM je, da se dobro obnese v primerih, ko imamo veliko število značilk. Še več, SVM zna najti \textit{iglo v kopici sena}. Zna torej v ogromnem naboru značilk poiskati tisto, ki najbolje odraža dinamiko modela. S pomočjo poskusov in napak je moč ob sistematičnem pregledu sicer neomejenega prostora izpeljanih značlik najti tiste, ki največ prispevajo k izboljšanju modela. Še ena lepa prednost metode SVM pred npr. nevronskimi mrežami je, da ne gre za \textit{črno škatlo}, pač pa je moč rezultate in končni SVM model interpretirati.

Glede na to, da globoke nevronske mreže nimajo višje izrazne vrednosti kot SVM obogaten z gornjimi izpeljanimi značilkami, je cilj razvoja algoritma, da se približa rezultatom pospešnih odločitvenih dreves in da preseže rezultate, o katerih so poročali drugi tekmovalci ($AMS > 2.8$).


\section{Generiranje novih značilk}
\comment{Poglavje 6 - ali so te količine dovolj?} \cite{Adam-Bourdarios14}

\section{Optimizacija AMS}

\section{Implementacija}
Metodo sem implementiral v programskem jeziku Python z uporabo paketa \texttt{scikit-learn}\cite{scikit-learn} za metodo SVM. Vsa koda, razvita in preizkušena v diplomskem delu, je dosegljiva na sistemu GitHub\footnote{\url{https://github.com/klemenkenda/HiggsML}}.


\section{Rezultati}




% -----------------------------------------------------------------------------
% POGLAVJE: Zaključek
% -----------------------------------------------------------------------------
\chapter*{Zaključek}
\addcontentsline{toc}{chapter}{Zaključek}

% -----------------------------------------------------------------------------
% ZAKLJUČNE STRANI
% -----------------------------------------------------------------------------
\input{endpages}

% -----------------------------------------------------------------------------
% DODATEK
% -----------------------------------------------------------------------------
\input{appendix}


\end{document}
